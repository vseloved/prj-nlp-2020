{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import en_core_web_md\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./query_res.json') as f:\n",
    "    dbpedia_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(title):\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            'exsectionformat': 'raw',\n",
    "        }\n",
    "    ).json()\n",
    "\n",
    "    pages = [x['extract'] for x in response['query']['pages'].values()]\n",
    "    html_text = '\\n'.join(pages)\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    p = [x.text for x in soup.findAll('p')]\n",
    "    q = [x.text for x in soup.findAll('blockquote')]\n",
    "    content = '\\n'.join(p + q)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# band_page_content = get_page_content('Katatonia')\n",
    "band_page_content = get_page_content('Black_Sabbath')\n",
    "# print(band_page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# todo: try Sentencizer\n",
    "# sents = [nlp(line) for line in re.split('(?<=\\w\\.{1}) ', band_page_content)]\n",
    "sents = [line for line in re.split('(?<=\\w\\.{1}) ', band_page_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ['EP'] list of supported record types\n",
    "\n",
    "def get_ents(doc):\n",
    "    return [(e.text, e.label_) for e in doc.ents]\n",
    "\n",
    "\n",
    "def get_ent(ents, t):\n",
    "    r = [x for x in ents if x[0] == t]\n",
    "    return r[0][1] if r else None\n",
    "\n",
    "def get_parent_verb_for_date(token):\n",
    "    if token.head and token.head.pos_ == 'VERB':\n",
    "        return token.head.text\n",
    "    return get_parent_verb_for_date(token.head)\n",
    "\n",
    "def check_head(token, text):\n",
    "    if token.head.text == token.text:\n",
    "        return token.head.text == text\n",
    "    return get_parent_verb_for_date(token.head)\n",
    "\n",
    "def get_albums(sent):\n",
    "    titles = re.findall('(([A-Z][a-z]+[\\s\\w\\.\\.\\.\\']+[A-Z][a-z]+)(?:\\s+\\(\\d+\\))?)', sent)\n",
    "    if titles:\n",
    "        yy = [x for x, _ in titles]\n",
    "        doc = nlp(sent)\n",
    "        org = [x for x in doc if x.ent_type_ == 'ORG']\n",
    "#         print('&&&&', org)\n",
    "        rrr = [x for x, _ in titles if any(org) in titles]\n",
    "#         print('*&****', rrr)\n",
    "            \n",
    "    return [x for x, _ in titles]\n",
    "    \n",
    "def has_num(sent):\n",
    "    return re.findall('\\d{4}', sent)\n",
    "\n",
    "def is_org(token):\n",
    "    is_curr_org = token.ent_type_ == 'ORG'\n",
    "    if not is_curr_org:\n",
    "        return False\n",
    "    is_begin_ent = token.ent_iob == 3\n",
    "    is_inside_ent = token.ent_iob == 1\n",
    "    if is_begin_ent:\n",
    "        return [is_org(x) for x in token.children]\n",
    "    else:\n",
    "        return is_org(token.head)\n",
    "    \n",
    "\n",
    "def parse_sent(doc):\n",
    "    for i in range(0, len(doc)):\n",
    "        token = doc[i]\n",
    "        if token.is_title == 'PROPN' or token.ent_type_ == 'ORG':\n",
    "\n",
    "\n",
    "def get_al_o(doc, als):\n",
    "    res = []\n",
    "    for token in doc:\n",
    "        if token.text in als and token.ent_type_ == 'ORG':\n",
    "            res.append(token.text)\n",
    "    return res\n",
    "\n",
    "def get_album_det(string):\n",
    "    res = re.findall('([A-Za-z\\'\\s]+)\\((\\d+)', string)\n",
    "    return res[0] if res else None\n",
    "                \n",
    "    \n",
    "# TODO: take all regex parsed 'titles', check doc for a sent if it contains title, check all words in title for: a) ORG, PERSON, PNOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ADP \n",
      "1991 NUM DATE\n",
      ", PUNCT \n",
      "Iommi PROPN PERSON\n",
      "and CCONJ \n",
      "Butler PROPN PERSON\n",
      "rejoined VERB \n",
      "Dio PROPN PERSON\n",
      "and CCONJ \n",
      "drummer NOUN \n",
      "Vinny PROPN PERSON\n",
      "Appice PROPN PERSON\n",
      "to PART \n",
      "record VERB \n",
      "Dehumanizer PROPN PERSON\n",
      "( PUNCT \n",
      "1992 NUM DATE\n",
      ") PUNCT \n",
      "!!! [{'title': 'Iommi and Butler rejoined Dio and drummer Vinny Appice to record Dehumanizer', 'year': '1992'}]\n"
     ]
    }
   ],
   "source": [
    "all_albums = []\n",
    "ss = sents[6:30]\n",
    "for i in range(len(ss)):\n",
    "    sent = ss[i]\n",
    "    has_n = has_num(sent)\n",
    "    if titles and has_n:\n",
    "        albums = get_albums(sent)\n",
    "        \n",
    "        for album in albums:\n",
    "            det = get_album_det(album)        \n",
    "            if det:\n",
    "                title, year = det\n",
    "                all_albums.append({'title': title.strip(), 'year': year})\n",
    "#         print(i, sent)\n",
    "#         t = parse_sent(nlp(sent))\n",
    "#         print('--', t)\n",
    "#         print('====')\n",
    "    \n",
    "#     print('ht', ht)\n",
    "#     parse_sent(nlp(sent))\n",
    "s = nlp(\"In 1991, Iommi and Butler rejoined Dio and drummer Vinny Appice to record Dehumanizer (1992)\")\n",
    "for t in s:\n",
    "    print(t.text, t.pos_, t.ent_type_)\n",
    "print('!!!', all_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
