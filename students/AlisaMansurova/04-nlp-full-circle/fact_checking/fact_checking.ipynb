{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import en_core_web_md\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dbpedia_query_res.json') as f:\n",
    "    dbpedia_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(title):\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            'exsectionformat': 'raw',\n",
    "        }\n",
    "    ).json()\n",
    "\n",
    "    pages = [x['extract'] for x in response['query']['pages'].values()]\n",
    "    html_text = '\\n'.join(pages)\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    p = [x.text for x in soup.findAll('p')]\n",
    "    q = [x.text for x in soup.findAll('blockquote')]\n",
    "    content = '\\n'.join(p + q)\n",
    "    return content\n",
    "\n",
    "\n",
    "def has_date(sent):\n",
    "    return re.search('\\\\d{4}', sent)\n",
    "\n",
    "\n",
    "def find_token_in_sent(sent, t):\n",
    "    res = [x for x in sent if x.lower_ == t.lower()]\n",
    "    return res[0] if res else None\n",
    "\n",
    "\n",
    "# Build in istitle() fn doesn't work for words with apostrophe ¯\\_(ツ)_/¯\n",
    "def is_title(string):\n",
    "    return string[0].isupper()\n",
    "\n",
    "\n",
    "def get_release_date(token):\n",
    "    year_match = re.search('^\\\\d{4}', token.text)\n",
    "    if year_match:\n",
    "        return year_match.group(0)\n",
    "    return next((x for x in (get_release_date(c) for c in token.children) if x), None)\n",
    "\n",
    "\n",
    "def get_album_name(token):\n",
    "    if token.ent_type == 'PERSON':\n",
    "        return None\n",
    "    children = [x for x in token.children if x.text\n",
    "                and '\\n' not in x.text\n",
    "                and x.ent_type_ != 'DATE']\n",
    "    child_texts = [x.text for x in children]\n",
    "\n",
    "    if is_title(token.text) or is_title(token.head.text):\n",
    "        res = [y for y in (get_album_name(x) for x in children) if y]\n",
    "        res_flattened = [x for sub in res for x in sub]\n",
    "        if (token.pos_ != 'AUX'\n",
    "                or token.pos_ == 'AUX' and 'album' not in child_texts) \\\n",
    "                and token.ent_type != 'PERSON':\n",
    "            res_flattened.append(token)\n",
    "        return sorted(res_flattened, key=lambda x: x.idx)\n",
    "\n",
    "    if 'album' in child_texts:\n",
    "        res = [get_album_name(x) for x in children if x.text != 'album']\n",
    "        res_flattened = [x for sub in [y for y in res if y] for x in sub if x]\n",
    "        return res_flattened\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_album_data_from_wiki_page(doc):\n",
    "    sents = list(doc.sents)\n",
    "    albums = []\n",
    "    for i in range(0, len(sents)):\n",
    "        sent = sents[i]\n",
    "        if (sent.text.find(' released') != -1 or sent.text.find('Released') != -1) \\\n",
    "                and has_date(sent.text):\n",
    "            first_token = sent[0]\n",
    "            album_name = None\n",
    "            released_token = find_token_in_sent(sent, 'released')\n",
    "            if first_token.pos_ == 'VERB' or first_token.pos_ == 'AUX' and i:\n",
    "                prev_text = sents[i - 1].text.strip()\n",
    "                a_names = get_album_name(sents[i - 1].root)\n",
    "                if a_names:\n",
    "                    album_name = ' '.join([x.text for x in a_names])\n",
    "            else:\n",
    "                a_names = get_album_name(sents[i].root)\n",
    "                if a_names:\n",
    "                    album_name = ' '.join([x.text for x in a_names])\n",
    "\n",
    "            if album_name:\n",
    "                album_name = ' '.join(\n",
    "                    [x for x in album_name.split(' ') if is_title(x) or len(x) < 4])\n",
    "                album_name = album_name.replace(\" ' \", \" 'n' \").replace(\n",
    "                    ' !', '!').replace(' ?', '?').replace(' .', '').replace(\n",
    "                        '(', '').replace(')', '').strip()\n",
    "                release_year = get_release_date(released_token)\n",
    "                year = release_year if type(\n",
    "                    release_year) == str else release_year.text if release_year else ''\n",
    "                albums.append({'album': album_name, 'release_years': year})\n",
    "\n",
    "                # Last resort\n",
    "                titles = re.findall(\n",
    "                    '([A-Z][a-z]+[\\\\s\\\\w\\\\.\\\\.\\\\.\\']+([A-Z][a-z]+)?(?:\\\\s+\\\\(\\\\d+\\\\)))', sent.text)\n",
    "                for v in titles:\n",
    "                    res = []\n",
    "                    parts = re.split('(\\\\(\\\\d+\\\\))', v[0])\n",
    "                    title = parts[0]\n",
    "                    year = parts[1].replace('(', '').replace(')', '')\n",
    "                    album_name = ' '.join([x for x in title.split(\n",
    "                        ' ') if x and (is_title(x) or len(x) < 4)])\n",
    "                    if album_name not in albums:\n",
    "                        albums.append(\n",
    "                            {'album': album_name, 'release_years': year})\n",
    "    return albums\n",
    "\n",
    "def get_results_from_wiki():\n",
    "    res = []\n",
    "    for page in actual_data:\n",
    "        doc = nlp(page)\n",
    "        albums = get_album_data_from_wiki_page(doc)\n",
    "        for album in albums:\n",
    "            existing = next((x for x in res if x['album'] == album['album']\n",
    "                             and x['release_years'] != album['release_years']), None)\n",
    "            if existing:\n",
    "                existing['release_years'] = f\"{existing['release_years']}|{album['release_years']}\"\n",
    "            res.append(album)\n",
    "    return sorted(res, key=lambda i: i['release_years'])\n",
    "\n",
    "\n",
    "def get_match_results(dbpedia_data, wiki_data):\n",
    "    match = []\n",
    "    for exp in dbpedia_data:\n",
    "        for wd in wiki_data:\n",
    "            if exp['album'] == wd['album'] and wd['album'] not in [x['album'] for x, _ in match]:\n",
    "                exp_years = exp['release_years'].split('|')\n",
    "                act_years = wd['release_years'].split('|')\n",
    "                if exp_years == act_years:\n",
    "                    match.append((wd, 1))\n",
    "                else:\n",
    "                    weight = round(\n",
    "                        len([x for x in act_years if x in exp_years])/len(exp_years), 2)\n",
    "                    match.append((wd, weight))\n",
    "    matched_album_names = [x['album'] for x, _ in match]\n",
    "    false_pos = [x for x in wiki_data if x['album'] not in matched_album_names]\n",
    "    false_neg = [x for x in dbpedia_data if x['album']\n",
    "                 not in matched_album_names]\n",
    "\n",
    "    match_score = sum([w for _, w in match])/len(match)\n",
    "\n",
    "    return {\n",
    "        'matched_perc': round(len(matched_album_names)/len(dbpedia_data) * match_score, 2),\n",
    "        'recall': round(len(matched_album_names)/(len(matched_album_names) + len(false_neg)), 2),\n",
    "        'precicion': round(len(matched_album_names)/(len(matched_album_names) + len(false_pos)), 2),\n",
    "        'false_pos_list': false_pos,\n",
    "        'false_neg_list': false_neg,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "band_page_content = get_page_content('Black_Sabbath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_q_data = [x for x in dbpedia_data['results']['bindings']]\n",
    "album_uris = [x['album_uri']['value'] for x in all_q_data]\n",
    "album_pages_content = [get_page_content(x.rsplit('/', 1)[-1]) for x in album_uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = album_pages_content\n",
    "actual_data.append(band_page_content)\n",
    "expected_data = [{k: v['value'] for k, v in x.items() if k != 'album_uri'} for x in all_q_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiki_data = get_results_from_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_match_results(expected_data, wiki_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
