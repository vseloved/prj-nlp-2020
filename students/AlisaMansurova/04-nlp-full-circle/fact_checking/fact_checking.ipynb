{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import en_core_web_md\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dbpedia_query_res.json') as f:\n",
    "    dbpedia_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(title):\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            'exsectionformat': 'raw',\n",
    "            'info': False, # TODO: make an arg\n",
    "        }\n",
    "    ).json()\n",
    "\n",
    "    pages = [x['extract'] for x in response['query']['pages'].values()]\n",
    "    html_text = '\\n'.join(pages)\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    p = [x.text for x in soup.findAll('p')]\n",
    "    q = [x.text for x in soup.findAll('blockquote')]\n",
    "    content = '\\n'.join(p + q)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def has_date(sent):\n",
    "    return re.search('\\d{4}', sent)\n",
    "\n",
    "def find_token_in_sent(sent, t):\n",
    "    res = [x for x in sent if x.lower_ == t.lower()]\n",
    "    return res[0] if res else None\n",
    "\n",
    "# Build istitle() fn doesn't work for words with apostrophe ¯\\_(ツ)_/¯\n",
    "def is_title(string):\n",
    "    return string[0].isupper()\n",
    "\n",
    "def get_release_date(token):\n",
    "    year_match = re.search('^\\d{4}', token.text)\n",
    "    if year_match:\n",
    "        return year_match.group(0)\n",
    "    return next((x for x in (get_release_date(c) for c in token.children) if x), None)\n",
    "\n",
    "    \n",
    "def get_album_name(token):\n",
    "    if token.ent_type == 'PERSON':\n",
    "        return None\n",
    "    children = [x for x in token.children if x.text \\\n",
    "                and '\\n' not in x.text \\\n",
    "                and x.pos_ != 'PUNCT' \\\n",
    "                and x.ent_type_ != 'DATE']\n",
    "    child_texts = [x.text for x in children]\n",
    "    \n",
    "    if is_title(token.text) or is_title(token.head.text):\n",
    "        res = [y for y in (get_album_name(x) for x in children) if y]\n",
    "        res_flattened = [x for sub in res for x in sub]\n",
    "        if (token.pos_ != 'AUX' \\\n",
    "            or token.pos_ == 'AUX' and 'album' not in child_texts) \\\n",
    "            and token.ent_type != 'PERSON':\n",
    "                res_flattened.append(token)\n",
    "        return sorted(res_flattened, key=lambda x: x.idx)\n",
    "\n",
    "    if 'album' in child_texts:       \n",
    "        res = [get_album_name(x) for x in children if x.text != 'album']\n",
    "        res_flattened = [x for sub in [y for y in res if y] for x in sub if x]\n",
    "        return res_flattened\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_album_data_from_wiki_page(doc):\n",
    "    sents = list(doc.sents)\n",
    "    albums = []\n",
    "    for i in range(0, len(sents)):\n",
    "        sent = sents[i]\n",
    "        if (sent.text.find(' released') != -1 or sent.text.find('Released') != -1) and has_date(sent.text):\n",
    "            first_token = sent[0]\n",
    "            album_name = None\n",
    "            released_token = find_token_in_sent(sent, 'released')\n",
    "            if first_token.pos_ == 'VERB' or first_token.pos_ == 'AUX' and i:\n",
    "                prev_text = sents[i - 1].text.strip()\n",
    "                a_names = get_album_name(sents[i - 1].root)\n",
    "                if a_names:\n",
    "                    album_name = ' '.join([x.text for x in a_names])\n",
    "            else:\n",
    "                a_names = get_album_name(sents[i].root)\n",
    "                if a_names:\n",
    "                    album_name = ' '.join([x.text for x in a_names])\n",
    "\n",
    "            if album_name:\n",
    "                album_name = ' '.join([x for x in album_name.split(' ') if is_title(x) or len(x) < 4])\n",
    "                album_name = album_name.replace(\" ' \", \" 'n' \") # it may be missed\n",
    "                release_year = get_release_date(released_token)\n",
    "                year = release_year if type(release_year) == str else release_year.text if release_year else ''\n",
    "                albums.append({'album': album_name, 'release_years': year})\n",
    "                \n",
    "                # Last resort\n",
    "                titles = re.findall('([A-Z][a-z]+[\\s\\w\\.\\.\\.\\']+([A-Z][a-z]+)?(?:\\s+\\(\\d+\\)))', sent.text)\n",
    "                for v in titles:\n",
    "                    res = []\n",
    "                    parts = re.split('(\\(\\d+\\))', v[0])\n",
    "                    title = parts[0]\n",
    "                    year = parts[1].replace('(', '').replace(')', '')\n",
    "                    album_name = ' '.join([x for x in title.split(' ') if x and (is_title(x) or len(x) < 4)])\n",
    "                    if album_name not in albums:\n",
    "                        albums.append({'album': album_name, 'release_years': year})\n",
    "    return albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# band_page_content = get_page_content('Black_Sabbath')\n",
    "with open('./bs.txt') as f:\n",
    "    band_page_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_q_data = [x for x in dbpedia_data['results']['bindings']]\n",
    "album_uris = [x['album_uri']['value'] for x in all_q_data]\n",
    "album_pages_content = [get_page_content(x.rsplit('/', 1)[-1]) for x in album_uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_pages_content = []\n",
    "for i in range(0, 36):\n",
    "    with open(f'./page_{i}.txt') as f:\n",
    "        album_pages_content.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = album_pages_content\n",
    "actual_data.append(band_page_content)\n",
    "expected_data = [{k: v['value'] for k, v in x.items() if k != 'album_uri'} for x in all_q_data]\n",
    "# print(expected_data)\n",
    "# print(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_results_from_wiki():\n",
    "    res = []\n",
    "    for page in actual_data:\n",
    "        doc = nlp(page)\n",
    "        albums = get_album_data_from_wiki_page(doc)\n",
    "        for album in albums:\n",
    "            existing = next((x for x in res if x['album'] == album['album'] and x['release_years'] != album['release_years']), None)\n",
    "            if existing:\n",
    "                existing['release_years'] = f\"{existing['release_years']}|{album['release_years']}\"\n",
    "            res.append(album)\n",
    "    return sorted(res, key = lambda i: i['release_years']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = get_results_from_wiki()\n",
    "# res = get_results_from_wiki_debug()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(res[0])\n",
    "# print(expected_data[0])\n",
    "match = []\n",
    "for exp in expected_data:\n",
    "    for r in res:\n",
    "        if exp['album'] == r['album'] and exp['release_years'] == r['release_years'] and r not in match:\n",
    "            match.append(r)\n",
    "\n",
    "false_pos = [x for x in res if x not in match]\n",
    "false_neg = [x for x in expected_data if x not in match]\n",
    "print(match, len(match))\n",
    "print('========')\n",
    "print('>>> false_pos', false_pos, len(false_pos))\n",
    "print('========')\n",
    "print('>>> false_neg', false_neg, len(false_neg))\n",
    "# print(expected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
