{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../tasks/02-structural-linguistics/data/headlines-test-set.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open('../../../../tasks/02-structural-linguistics/data/examiner-headlines.txt', 'r') as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headline(doc):\n",
    "    res = ''\n",
    "\n",
    "    valid_pos = ['NOUN', 'VERB', 'AUX', 'PRON', 'ADJ', 'ADV', 'SCONJ']\n",
    "    invalid_pos = ['DET', 'CONJ', 'CCONJ', 'PART', 'INTJ', 'ADP']\n",
    "    \n",
    "    sent_len = len(doc) - 1\n",
    "\n",
    "    for token in doc:\n",
    "        prev_token = doc[token.i - 1]\n",
    "        next_token = doc[token.i + 1] if sent_len > token.i else None\n",
    "        \n",
    "        is_part_of_hyphened = prev_token.text == '-' and not prev_token.whitespace_ \\\n",
    "            or next_token and next_token.text == '-' and not next_token.whitespace_\n",
    "        is_start_of_quote = prev_token.is_quote and not prev_token.whitespace_\n",
    "        is_last = token.i == sent_len or token.i == sent_len - 1 and doc[sent_len].is_punct\n",
    "        is_adp = token.pos_ == 'SCONJ' and \\\n",
    "            any(c.pos_ == 'NOUN' or c.pos_ == 'ADP' or c.pos_ == 'PROPN' for c in token.children)\n",
    "        is_pron = token.lemma_ == '-PRON-' or token.pos_ == 'DET' and token.head.pos_ == 'PRON'\n",
    "        is_det_title = token.pos_ == 'DET' and prev_token.pos_ == 'PUNCT'\n",
    "        is_propn_to_capitalize = len(token.text) <= 3 and token.pos_ == 'PROPN' and token.is_lower\n",
    "        is_propn_to_skip = token.pos_ == 'PROPN' and not token.is_lower\n",
    "        is_neg_adv = token.text.lower() == 'not' and \\\n",
    "            (token.head.pos_ == 'AUX' or token.head.pos_ == 'VERB' or token.head.pos_ == 'ADJ')\n",
    "\n",
    "        should_capitalize = (\n",
    "                len(token.text) > 3\n",
    "                or ((token.is_sent_start or is_last) \\\n",
    "                    and not token.is_punct and not token.is_quote\n",
    "                ) or is_part_of_hyphened \\\n",
    "                or is_start_of_quote \\\n",
    "                or token.pos_ in valid_pos \\\n",
    "                or is_pron or is_det_title \\\n",
    "                or is_propn_to_capitalize \\\n",
    "                or is_neg_adv\n",
    "            ) and not token.is_upper and not (len(token.text) <=3 and is_adp) and not is_propn_to_skip\n",
    "\n",
    "        if should_capitalize:\n",
    "            res += token.text.capitalize()\n",
    "        elif token.pos_ in invalid_pos:\n",
    "            res += token.text.lower()\n",
    "        else:\n",
    "            res += token.text\n",
    "        res += token.whitespace_\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data):\n",
    "    ok = 0\n",
    "\n",
    "    for inp, exp in data:\n",
    "        doc = nlp(inp)\n",
    "        if (headline(doc) == exp):\n",
    "            ok += 1\n",
    "\n",
    "    return ok/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_corpus(corpus):\n",
    "    ok = 0\n",
    "    for line in corpus:\n",
    "        formatted = headline(nlp(line))\n",
    "        if formatted == line:\n",
    "            ok += 1\n",
    "    return ok, ok/len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def debug(corpus, n):\n",
    "    random.shuffle(corpus)\n",
    "    for sample in corpus[:n]:\n",
    "        print(sample)\n",
    "        print(headline(nlp(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug(corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 0.128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
