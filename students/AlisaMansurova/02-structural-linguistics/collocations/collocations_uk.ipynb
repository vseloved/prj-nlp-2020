{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/lissm/stanfordnlp_resources/uk_iu_models/uk_iu_tokenizer.pt', 'lang': 'uk', 'shorthand': 'uk_iu', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/lissm/stanfordnlp_resources/uk_iu_models/uk_iu_tagger.pt', 'pretrain_path': '/Users/lissm/stanfordnlp_resources/uk_iu_models/uk_iu.pretrain.pt', 'lang': 'uk', 'shorthand': 'uk_iu', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/lissm/stanfordnlp_resources/uk_iu_models/uk_iu_lemmatizer.pt', 'lang': 'uk', 'shorthand': 'uk_iu', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(lang='uk', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../tasks/02-structural-linguistics/data/tyhrolovy.txt') as f:\n",
    "    corpus = [l for l in (line.strip() for line in f) if l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_pairs(doc):\n",
    "    pairs = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            anim = 'Animacy=Anim' in word.feats\n",
    "            if anim:\n",
    "                adj = next((x for x in sent.words if int(x.index) == int(word.index) - 1 and x.upos == 'ADJ'), None)\n",
    "                if adj:\n",
    "                    pairs.append(f'{adj.lemma.lower()} {word.lemma.lower()}')\n",
    "    return pairs\n",
    "\n",
    "def order_pairs(pairs):\n",
    "    with_freq = {x: pairs.count(x) for x in pairs}\n",
    "    return [f'{v}: {k}' for k, v in sorted(with_freq.items(), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def do_find(corpus):\n",
    "    res = []\n",
    "    for prgr in corpus:\n",
    "        doc = nlp(prgr)\n",
    "        pairs = find_pairs(doc)\n",
    "        if pairs:\n",
    "            res += pairs\n",
    "\n",
    "    for x in order_pairs(res):\n",
    "        print(x)\n",
    "\n",
    "\n",
    "def find_anim_adj_debug(corpus, s, e):\n",
    "    do_find(corpus[s:e])\n",
    "\n",
    "\n",
    "def find_anim_adj_all(corpus):\n",
    "    do_find(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: легендарний дракон\n",
      "1: справжній дракон\n",
      "1: вогненноока голова\n",
      "1: двоокий циклоп\n",
      "1: живий мертвяк\n"
     ]
    }
   ],
   "source": [
    "find_anim_adj_debug(corpus, 10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
