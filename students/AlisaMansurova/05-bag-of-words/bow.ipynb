{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pprint\n",
    "from math import log\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from langdetect import detect\n",
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./rozetka_all.json') as f:\n",
    "    reviews = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = [r for r in reviews if r \\\n",
    "               and (r['text'].strip() and r['rating'] \\\n",
    "                or r.get('pros', '').strip() or r.get('cons', '').strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ukr_reviews = []\n",
    "for review in all_reviews:\n",
    "    text = review['text']\n",
    "    pros = review.get('pros')\n",
    "    cons = review.get('cons')\n",
    "\n",
    "    try:\n",
    "        t_lang = detect(text)\n",
    "        p_lang = detect(text) if pros else t_lang\n",
    "        c_lang = detect(text) if cons else t_lang\n",
    "        if t_lang == 'uk' and p_lang == 'uk' and c_lang == 'uk':\n",
    "            ukr_reviews.append(review)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed once\n",
    "# with open('./rozetka_uk.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(ukr_reviews, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "with open('./rozetka_uk.json') as f:\n",
    "    ukr_reviews = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, text_processor):\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def get_features(self):\n",
    "        feature_words = []\n",
    "\n",
    "        for review in self.X_data:\n",
    "            processed = self.text_processor(review['text'])\n",
    "\n",
    "            for word in processed:\n",
    "                if word not in feature_words:\n",
    "                    feature_words.append(word)\n",
    "\n",
    "        features = {x: i for i, x in enumerate(feature_words)}\n",
    "        features.update({'UNK': len(features)})\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_senses(self):\n",
    "        pos = []\n",
    "        neg = []\n",
    "        neut = []\n",
    "\n",
    "        for review in self.X_data:\n",
    "            text = self.text_processor(review['text'])\n",
    "            sens = review['sens']\n",
    "\n",
    "            if sens == 'neut':\n",
    "                for t in text:\n",
    "\n",
    "                    word = t\n",
    "                    neut.append(word)\n",
    "            elif sens == 'pos':\n",
    "                for t in text:\n",
    "                    word = t\n",
    "                    pos.append(word)\n",
    "            else:\n",
    "                for t in text:\n",
    "                    word = t\n",
    "                    neg.append(word)\n",
    "        return pos, neg, neut\n",
    "\n",
    "    def get_feature_counts_by_class(self):\n",
    "        pos, neg, neut = self.get_senses()\n",
    "        features = self.get_features()\n",
    "\n",
    "        count = {'pos': [], 'neg': [], 'neut': []}\n",
    "        cnt_pos = Counter(pos)\n",
    "        cnt_neg = Counter(neg)\n",
    "        cnt_neut = Counter(neut)\n",
    "\n",
    "        for w in features.keys():\n",
    "            pos_c = cnt_pos[w]\n",
    "            neg_c = cnt_neg[w]\n",
    "            neut_c = cnt_neut[w]\n",
    "            count['pos'].append(pos_c)\n",
    "            count['neg'].append(neg_c)\n",
    "            count['neut'].append(neut_c)\n",
    "        return count\n",
    "\n",
    "    def get_feature_weights_by_class(self):\n",
    "        res = {}\n",
    "        feat_counts = self.get_feature_counts_by_class()\n",
    "\n",
    "        for k, v in feat_counts.items():\n",
    "            res[k] = [log(x/len(v)) if x else log(0.1/len(v)) for x in v]\n",
    "        return res\n",
    "\n",
    "    def calculate_bias_by_class(self):\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        neut = 0\n",
    "        all_count = len(self.X_data)\n",
    "\n",
    "        for review in self.X_data:\n",
    "            sens = review['sens']\n",
    "\n",
    "            if sens == 'neut':\n",
    "                neut += 1\n",
    "            elif sens == 'pos':\n",
    "                pos += 1\n",
    "            else:\n",
    "                neg += 1\n",
    "\n",
    "        return {'pos': round(log(pos/all_count), 4),\n",
    "                'neg': round(log(neg/all_count), 4),\n",
    "                'neut': round(log(neut/all_count), 4)\n",
    "                }\n",
    "\n",
    "    def predict_class(self, text, weights, bias):\n",
    "        text_words = self.text_processor(text)\n",
    "        features = self.features\n",
    "\n",
    "        p_pos = bias['pos'] + sum(weights['pos'][features.get(\n",
    "            word, features['UNK'])] for word in text_words)\n",
    "        p_neg = bias['neg'] + sum(weights['neg'][features.get(\n",
    "            word, features['UNK'])] for word in text_words)\n",
    "        p_neut = bias['neut'] + sum(weights['neut'][features.get(\n",
    "            word, features['UNK'])] for word in text_words)\n",
    "\n",
    "        p_dict = {'pos': p_pos, 'neg': p_neg, 'neut': p_neut}\n",
    "\n",
    "        return max(p_dict, key=p_dict.get)\n",
    "\n",
    "    def fit(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        self.bias = self.calculate_bias_by_class()\n",
    "        self.weights = self.get_feature_weights_by_class()\n",
    "\n",
    "    def predict(self, y_data):\n",
    "        res = []\n",
    "\n",
    "        for review in y_data:\n",
    "            text = review['text']\n",
    "            pros = review.get('pros', '')\n",
    "            cons = review.get('cons', '')\n",
    "            all_text = text + pros + cons\n",
    "            res.append(self.predict_class(all_text, self.weights, self.bias))\n",
    "        return res\n",
    "\n",
    "\n",
    "def get_corpus(reviews):\n",
    "    res = []\n",
    "    for review in reviews:\n",
    "        rating = review['rating']\n",
    "        text = review['text']\n",
    "        pros = review.get('pros')\n",
    "        cons = review.get('cons')\n",
    "\n",
    "        rev_text = {'text': text}\n",
    "        rev_pros = {'text': pros, 'sens': 'pos'} if pros else None\n",
    "        rev_cons = {'text': cons, 'sens': 'neg'} if cons else None\n",
    "\n",
    "        if rating:\n",
    "            if rating == 5:\n",
    "                rev_text['sens'] = 'pos'\n",
    "            elif rating >= 3:\n",
    "                rev_text['sens'] = 'neut'\n",
    "            else:\n",
    "                rev_text['sens'] = 'neg'\n",
    "            res.append(rev_text)\n",
    "\n",
    "        if rev_pros:\n",
    "            res.append(rev_pros)\n",
    "\n",
    "        if rev_cons:\n",
    "            res.append(rev_cons)\n",
    "\n",
    "    pos = [x for x in res if x['sens'] == 'pos']\n",
    "    neg = [x for x in res if x['sens'] == 'neg']\n",
    "    neut = [x for x in res if x['sens'] == 'neut']\n",
    "    min_len = min(len(pos), len(neg), len(neut))\n",
    "    pos_f = pos[:min_len]\n",
    "    neg_f = neg[:min_len]\n",
    "    neut_f = neut[:min_len]\n",
    "\n",
    "    res_normalized = pos_f + neg_f + neut_f\n",
    "    print('Lenght of each cohort:', min_len)\n",
    "    shuffle(res_normalized)\n",
    "    return res_normalized\n",
    "\n",
    "\n",
    "def divide_data(data):\n",
    "    data_len = int(len(data) * 0.7)\n",
    "    return data[:data_len], data[data_len:]\n",
    "\n",
    "\n",
    "\"\"\" text processors START \"\"\"\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "\n",
    "\n",
    "def lowerize_text(text):\n",
    "    return [word.lower() for word in tokenize_text(text)]\n",
    "\n",
    "\n",
    "def _lemmatize(text):\n",
    "    res = []\n",
    "    for word in tokenize_text(text):\n",
    "        m_word = morph.parse(word)[0]\n",
    "        res.append((m_word.normal_form, m_word))\n",
    "    return res\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [x for x, _ in _lemmatize(text)]\n",
    "\n",
    "\n",
    "def filterize_text(text):\n",
    "    res = []\n",
    "    lemmatized = _lemmatize(text)\n",
    "\n",
    "    symbols = ['-', '+', ':', '<', '>', '&']\n",
    "    invalid_pos = ['CONJ', 'INTJ', 'PREP', 'NPRO']\n",
    "    invalid_non_oc_pos = ['NUMB,intg', 'NUMB,real', 'ROMN', 'PNCT', 'LATN']\n",
    "\n",
    "    for word, m_word in lemmatized:\n",
    "        if len(word) and str(m_word.tag) not in invalid_non_oc_pos and \\\n",
    "                m_word.tag.POS not in invalid_pos and \\\n",
    "                word not in symbols:\n",
    "            res.append(word)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def negatiaze_text(text):\n",
    "    res = []\n",
    "    words = filterize_text(text)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        p = morph.parse(word)[0]\n",
    "        if (p.tag.POS == 'ADJF' or p.tag.POS == 'VERB' or p.tag.POS == 'INFN') \\\n",
    "                and words[i-1] == 'не':\n",
    "            res.append(f'не_{word}')\n",
    "        else:\n",
    "            res.append(word)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def filterize_text_q(text):\n",
    "    res = []\n",
    "    if text.endswith('?'):\n",
    "        return []\n",
    "    return negatiaze_text(text)\n",
    "\n",
    "\n",
    "def ngrammaze_text(text, additional_preproc=None):\n",
    "    if additional_preproc:\n",
    "        words = ' '.join(additional_preproc(text))\n",
    "    else:\n",
    "        words = text\n",
    "    return [words[i:i + 3] if i > 0 else '^' + words[i:i + 3] for i in range(0, len(words), 1)]\n",
    "\n",
    "\n",
    "\"\"\" text processors END \"\"\"\n",
    "\n",
    "\n",
    "def get_X(reviews):\n",
    "    return [x['text'] for x in reviews]\n",
    "\n",
    "\n",
    "def get_y(reviews):\n",
    "    return [x['sens'] for x in reviews]\n",
    "\n",
    "\n",
    "def get_classification_report(preprocess_fn, train_data, test_data, y_target):\n",
    "    cls = NaiveBayesClassifier(preprocess_fn)\n",
    "    cls.fit(train_data)\n",
    "    test_predict = cls.predict(test_data)\n",
    "    print(classification_report(y_target, test_predict))\n",
    "\n",
    "\n",
    "def get_cross_validation_report(preprocess_fn, X_train, y_train, X_test, y_target):\n",
    "    vect = CountVectorizer(tokenizer=preprocess_fn)\n",
    "    cls_1 = Pipeline([('vect', vect), ('cls', MultinomialNB())])\n",
    "    cls_1.fit(X_train, y_train)\n",
    "    cls_1.predict\n",
    "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "               'precision': make_scorer(precision_score, average='macro'),\n",
    "               'recall': make_scorer(recall_score, average='macro'),\n",
    "               'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "               'f1_weighted': make_scorer(f1_score, average='weighted')}\n",
    "    res = cross_validate(cls_1, X_test, y_target, return_train_score=True)\n",
    "    pp = pprint.PrettyPrinter(indent=4, compact=True)\n",
    "    pp.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each cohort: 1080\n"
     ]
    }
   ],
   "source": [
    "corpus = get_corpus(ukr_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = divide_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = get_X(train_data)\n",
    "X_test = get_X(test_data)\n",
    "y_train = get_y(train_data)\n",
    "y_target = get_y(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.34      0.50       326\n",
      "        neut       0.41      0.98      0.57       315\n",
      "         pos       0.81      0.22      0.35       331\n",
      "\n",
      "    accuracy                           0.51       972\n",
      "   macro avg       0.71      0.51      0.47       972\n",
      "weighted avg       0.71      0.51      0.47       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([0.03127098, 0.02997279, 0.03269792, 0.03125978, 0.03050375]),\n",
      "    'score_time': array([0.00904202, 0.00991797, 0.00869703, 0.01004219, 0.01065016]),\n",
      "    'test_accuracy': array([0.62564103, 0.68717949, 0.56185567, 0.58247423, 0.61340206]),\n",
      "    'test_f1_macro': array([0.62737363, 0.67610681, 0.5542328 , 0.58058608, 0.61049818]),\n",
      "    'test_f1_weighted': array([0.62737601, 0.67640747, 0.55340779, 0.57989691, 0.61025254]),\n",
      "    'test_precision': array([0.70160727, 0.74371629, 0.59736145, 0.63728938, 0.72633053]),\n",
      "    'test_recall': array([0.62890813, 0.6895899 , 0.56510157, 0.58553669, 0.61697192])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.92      0.36      0.52       326\n",
      "        neut       0.40      0.97      0.56       315\n",
      "         pos       0.75      0.18      0.28       331\n",
      "\n",
      "    accuracy                           0.49       972\n",
      "   macro avg       0.69      0.50      0.46       972\n",
      "weighted avg       0.69      0.49      0.45       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lowerize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([0.03264093, 0.03215694, 0.03589129, 0.03449416, 0.03251028]),\n",
      "    'score_time': array([0.00977206, 0.01093817, 0.00927997, 0.01086593, 0.01020694]),\n",
      "    'test_accuracy': array([0.65641026, 0.66666667, 0.57731959, 0.60309278, 0.59278351]),\n",
      "    'test_f1_macro': array([0.65861142, 0.6521153 , 0.57005177, 0.60366023, 0.58953623]),\n",
      "    'test_f1_weighted': array([0.65873464, 0.65236794, 0.56937127, 0.60289281, 0.58921748]),\n",
      "    'test_precision': array([0.7202701 , 0.74380872, 0.62855841, 0.63705853, 0.68900585]),\n",
      "    'test_recall': array([0.65945166, 0.66923016, 0.58081178, 0.60564621, 0.5959707 ])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lowerize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.94      0.36      0.52       326\n",
      "        neut       0.39      0.99      0.56       315\n",
      "         pos       0.81      0.14      0.24       331\n",
      "\n",
      "    accuracy                           0.49       972\n",
      "   macro avg       0.71      0.50      0.44       972\n",
      "weighted avg       0.72      0.49      0.44       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([1.47227502, 1.50447416, 1.57949591, 1.44217491, 1.43997693]),\n",
      "    'score_time': array([0.35878015, 0.41877699, 0.29701304, 0.407794  , 0.40071225]),\n",
      "    'test_accuracy': array([0.69230769, 0.65641026, 0.61340206, 0.59793814, 0.64948454]),\n",
      "    'test_f1_macro': array([0.69847591, 0.64840349, 0.60594441, 0.6030658 , 0.64863941]),\n",
      "    'test_f1_weighted': array([0.69922169, 0.64838907, 0.60515513, 0.60295841, 0.64883207]),\n",
      "    'test_precision': array([0.74565286, 0.73095238, 0.65353813, 0.64513187, 0.71585567]),\n",
      "    'test_recall': array([0.69432419, 0.65910739, 0.61687942, 0.5998742 , 0.65103785])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.94      0.37      0.53       326\n",
      "        neut       0.41      0.97      0.57       315\n",
      "         pos       0.83      0.24      0.38       331\n",
      "\n",
      "    accuracy                           0.52       972\n",
      "   macro avg       0.72      0.53      0.49       972\n",
      "weighted avg       0.73      0.52      0.49       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(filterize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([1.57367492, 1.50916982, 1.59950614, 1.48378277, 1.44924498]),\n",
      "    'score_time': array([0.35865712, 0.42318201, 0.30774689, 0.38333893, 0.40724301]),\n",
      "    'test_accuracy': array([0.6974359 , 0.68717949, 0.58762887, 0.62886598, 0.65979381]),\n",
      "    'test_f1_macro': array([0.70146877, 0.67970203, 0.58510331, 0.63496668, 0.66175363]),\n",
      "    'test_f1_weighted': array([0.70220932, 0.67911327, 0.58468232, 0.63551453, 0.66208518]),\n",
      "    'test_precision': array([0.72576119, 0.72463261, 0.60462448, 0.65869299, 0.69563724]),\n",
      "    'test_recall': array([0.6986532 , 0.68878562, 0.58962519, 0.62944833, 0.66032486])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(filterize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.43      0.58       326\n",
      "        neut       0.42      0.97      0.59       315\n",
      "         pos       0.82      0.24      0.37       331\n",
      "\n",
      "    accuracy                           0.54       972\n",
      "   macro avg       0.72      0.55      0.51       972\n",
      "weighted avg       0.73      0.54      0.51       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(negatiaze_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([2.62964416, 2.51945972, 2.69767189, 2.54449725, 2.60344887]),\n",
      "    'score_time': array([0.5982008 , 0.74097037, 0.51896906, 0.68202972, 0.71885109]),\n",
      "    'test_accuracy': array([0.7025641 , 0.69230769, 0.63402062, 0.62886598, 0.67010309]),\n",
      "    'test_f1_macro': array([0.70715409, 0.68512428, 0.63190089, 0.63364405, 0.67314222]),\n",
      "    'test_f1_weighted': array([0.70791464, 0.68448238, 0.63140784, 0.63407864, 0.67367853]),\n",
      "    'test_precision': array([0.73195539, 0.7181338 , 0.65098145, 0.65975469, 0.70936149]),\n",
      "    'test_recall': array([0.7037037 , 0.69391383, 0.63618974, 0.62977393, 0.67050357])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(negatiaze_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.92      0.44      0.60       326\n",
      "        neut       0.43      0.97      0.59       315\n",
      "         pos       0.83      0.24      0.38       331\n",
      "\n",
      "    accuracy                           0.55       972\n",
      "   macro avg       0.73      0.55      0.52       972\n",
      "weighted avg       0.73      0.55      0.52       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(filterize_text_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([2.64487386, 2.43449402, 2.63043094, 2.52366281, 2.50279093]),\n",
      "    'score_time': array([0.58888984, 0.72793293, 0.52039123, 0.6685791 , 0.69508004]),\n",
      "    'test_accuracy': array([0.70769231, 0.67692308, 0.62371134, 0.63917526, 0.65979381]),\n",
      "    'test_f1_macro': array([0.71225899, 0.67184372, 0.62254615, 0.64317665, 0.66276134]),\n",
      "    'test_f1_weighted': array([0.71317409, 0.67140836, 0.62207051, 0.64362264, 0.66335936]),\n",
      "    'test_precision': array([0.73216447, 0.70529334, 0.63796959, 0.66586662, 0.70138017]),\n",
      "    'test_recall': array([0.70827321, 0.67772493, 0.62560773, 0.63987494, 0.66008436])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(filterize_text_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.38      0.53       326\n",
      "        neut       0.40      1.00      0.57       315\n",
      "         pos       0.82      0.12      0.21       331\n",
      "\n",
      "    accuracy                           0.49       972\n",
      "   macro avg       0.70      0.50      0.44       972\n",
      "weighted avg       0.71      0.49      0.44       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, lowerize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([0.08786774, 0.08607125, 0.09123993, 0.08525681, 0.08237219]),\n",
      "    'score_time': array([0.02169704, 0.0260098 , 0.02031374, 0.0247829 , 0.02431417]),\n",
      "    'test_accuracy': array([0.67179487, 0.66153846, 0.67525773, 0.64948454, 0.65979381]),\n",
      "    'test_f1_macro': array([0.67307976, 0.65487451, 0.67322107, 0.65391523, 0.65991654]),\n",
      "    'test_f1_weighted': array([0.67331337, 0.65412287, 0.67262047, 0.65382265, 0.65994816]),\n",
      "    'test_precision': array([0.69669988, 0.67592593, 0.68595952, 0.67169162, 0.6933808 ]),\n",
      "    'test_recall': array([0.67364117, 0.66421616, 0.67715248, 0.65060495, 0.66096866])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, lowerize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.37      0.52       326\n",
      "        neut       0.39      1.00      0.56       315\n",
      "         pos       0.86      0.09      0.17       331\n",
      "\n",
      "    accuracy                           0.48       972\n",
      "   macro avg       0.72      0.49      0.42       972\n",
      "weighted avg       0.72      0.48      0.41       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, lemmatize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([1.56223011, 1.4749372 , 1.60342288, 1.51111603, 1.48056793]),\n",
      "    'score_time': array([0.3701551 , 0.43318677, 0.31703329, 0.39748788, 0.42143583]),\n",
      "    'test_accuracy': array([0.68717949, 0.6974359 , 0.68556701, 0.61340206, 0.67525773]),\n",
      "    'test_f1_macro': array([0.69005721, 0.69372439, 0.68320226, 0.61746566, 0.6772963 ]),\n",
      "    'test_f1_weighted': array([0.69055473, 0.6935486 , 0.68283533, 0.61727617, 0.67749621]),\n",
      "    'test_precision': array([0.71137351, 0.71355813, 0.70406273, 0.64838136, 0.70696794]),\n",
      "    'test_recall': array([0.68855219, 0.69918539, 0.68773449, 0.61510342, 0.67635328])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, lemmatize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.95      0.30      0.46       326\n",
      "        neut       0.37      1.00      0.54       315\n",
      "         pos       0.86      0.06      0.11       331\n",
      "\n",
      "    accuracy                           0.44       972\n",
      "   macro avg       0.73      0.45      0.37       972\n",
      "weighted avg       0.73      0.44      0.37       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, filterize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([1.59539604, 1.47582912, 1.58996987, 1.50018907, 1.51606607]),\n",
      "    'score_time': array([0.35130596, 0.44019294, 0.31457806, 0.40962577, 0.43612194]),\n",
      "    'test_accuracy': array([0.64615385, 0.68717949, 0.6443299 , 0.6185567 , 0.67525773]),\n",
      "    'test_f1_macro': array([0.64652442, 0.68515896, 0.64483218, 0.62213076, 0.67654041]),\n",
      "    'test_f1_weighted': array([0.64681345, 0.68513503, 0.64452926, 0.62226185, 0.67688957]),\n",
      "    'test_precision': array([0.65070644, 0.70793871, 0.6542624 , 0.63674524, 0.70091324]),\n",
      "    'test_recall': array([0.64670515, 0.6882875 , 0.64540645, 0.61934732, 0.67515078])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, filterize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.95      0.30      0.45       326\n",
      "        neut       0.37      1.00      0.54       315\n",
      "         pos       0.87      0.06      0.11       331\n",
      "\n",
      "    accuracy                           0.44       972\n",
      "   macro avg       0.73      0.45      0.37       972\n",
      "weighted avg       0.74      0.44      0.37       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, filterize_text_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([2.63989401, 2.51032233, 2.7345171 , 2.55634475, 2.54128098]),\n",
      "    'score_time': array([0.59502196, 0.72793388, 0.54447985, 0.69707012, 0.73798299]),\n",
      "    'test_accuracy': array([0.66666667, 0.67692308, 0.66494845, 0.62886598, 0.67525773]),\n",
      "    'test_f1_macro': array([0.66874698, 0.67664768, 0.66498165, 0.63200937, 0.67717524]),\n",
      "    'test_f1_weighted': array([0.66946274, 0.67670002, 0.66453685, 0.63231631, 0.67764922]),\n",
      "    'test_precision': array([0.6748158 , 0.697263  , 0.67251884, 0.64078624, 0.70504148]),\n",
      "    'test_recall': array([0.66666667, 0.67754269, 0.66624487, 0.62912273, 0.67515078])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, filterize_text_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.95      0.30      0.45       326\n",
      "        neut       0.37      1.00      0.54       315\n",
      "         pos       0.86      0.06      0.11       331\n",
      "\n",
      "    accuracy                           0.44       972\n",
      "   macro avg       0.73      0.45      0.37       972\n",
      "weighted avg       0.73      0.44      0.36       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, negatiaze_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([2.72987318, 2.55923009, 2.76168394, 2.63325906, 2.57849503]),\n",
      "    'score_time': array([0.62254691, 0.73774409, 0.52852488, 0.724365  , 0.71830297]),\n",
      "    'test_accuracy': array([0.66153846, 0.68717949, 0.65979381, 0.62371134, 0.67010309]),\n",
      "    'test_f1_macro': array([0.66237509, 0.68549109, 0.66021975, 0.62683532, 0.6724804 ]),\n",
      "    'test_f1_weighted': array([0.66282558, 0.68541464, 0.65989767, 0.62700816, 0.67285759]),\n",
      "    'test_precision': array([0.66878283, 0.71094218, 0.66818983, 0.63701997, 0.70065551]),\n",
      "    'test_recall': array([0.66209716, 0.6882875 , 0.66095386, 0.62431272, 0.67010027])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, negatiaze_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.41      0.57       326\n",
      "        neut       0.41      1.00      0.58       315\n",
      "         pos       0.80      0.15      0.25       331\n",
      "\n",
      "    accuracy                           0.51       972\n",
      "   macro avg       0.70      0.52      0.47       972\n",
      "weighted avg       0.71      0.51      0.46       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(ngrammaze_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([0.06954122, 0.06615901, 0.07011938, 0.06650329, 0.06421614]),\n",
      "    'score_time': array([0.01671195, 0.02056694, 0.01598072, 0.01860404, 0.01903796]),\n",
      "    'test_accuracy': array([0.66153846, 0.64615385, 0.67525773, 0.6443299 , 0.62886598]),\n",
      "    'test_f1_macro': array([0.66294789, 0.63373402, 0.67370659, 0.64511197, 0.62674173]),\n",
      "    'test_f1_weighted': array([0.66330093, 0.63274974, 0.67322756, 0.64468967, 0.62647224]),\n",
      "    'test_precision': array([0.68305041, 0.66315525, 0.69547157, 0.67935587, 0.67596331]),\n",
      "    'test_recall': array([0.66305916, 0.64962611, 0.67747068, 0.64676435, 0.63115403])}\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(ngrammaze_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
