{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1757,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import en_core_web_md\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknowns(data):\n",
    "    return [x for x in data if x['gold_label'] != '-']\n",
    "\n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_train.jsonl') as f:\n",
    "    train_data = filter_unknowns([json.loads(line) for line in f.readlines()])\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_dev.jsonl') as f:\n",
    "    dev_data = filter_unknowns([json.loads(line) for line in f.readlines()])\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_test.jsonl') as f:\n",
    "    test_data = filter_unknowns([json.loads(line) for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1892,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Utils '''\n",
    "\n",
    "\n",
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            res.update(f(*arg))\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "\n",
    "def filter_stop_words(doc):\n",
    "    return [x for x in doc if not (x.pos_ == 'DET' or x.pos_ == 'NUM'\n",
    "                                   or x.is_stop and x.dep_ != 'ROOT')]\n",
    "\n",
    "\n",
    "def normalize_sent(func):\n",
    "    def inner(s1, s2):\n",
    "        return func(filter_stop_words(s1), filter_stop_words(s2))\n",
    "    return inner\n",
    "\n",
    "\n",
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def get_intersection(ents1, ents2):\n",
    "    setA = set(ents1)\n",
    "    setB = set(ents2)\n",
    "    universe = setA | setB\n",
    "    if not setB:\n",
    "        return 'NONE'\n",
    "\n",
    "    return len(setA & setB)/(len(setB))\n",
    "\n",
    "\n",
    "def get_tokens_similarity(toks1, toks2):\n",
    "    setA = set(toks1)\n",
    "    setB = set(toks2)\n",
    "    universe = set(toks1) | set(toks2)\n",
    "    sim = [x.similarity(y)\n",
    "           for x in setA for y in setB if x.has_vector and y.has_vector]\n",
    "    return len(sim)/(len(universe))\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Working with concepts'''\n",
    "\n",
    "\n",
    "def get_concepts_for_roots(data):\n",
    "    ex_conc = []\n",
    "\n",
    "    def get_concepts_for_sent(sent):\n",
    "        s_conc = None\n",
    "        if os.path.isfile('./concs.txt'):\n",
    "            with open('./concs.txt') as f:\n",
    "                ex_conc = [x.rstrip() for x in f.readlines()]\n",
    "        else:\n",
    "            ex_conc = []\n",
    "        for tok in nlp(sent):\n",
    "            if tok.lemma_ not in ex_conc and tok.dep_ == 'ROOT':\n",
    "                s_conc = get_concepts(tok.lemma_)['edges']\n",
    "                with open('./concs.txt', 'a') as f:\n",
    "                    f.write(tok.lemma_ + '\\n')\n",
    "                ex_conc.append(tok.lemma_)\n",
    "\n",
    "        return s_conc\n",
    "\n",
    "    conc = []\n",
    "    for i, item in enumerate(data):\n",
    "        conc.append(get_concepts_for_sent(item['sentence1']))\n",
    "        conc.append(get_concepts_for_sent(item['sentence2']))\n",
    "    return conc\n",
    "\n",
    "\n",
    "def get_concepts(concept):\n",
    "    offset = 0\n",
    "    req = requests.get('http://api.conceptnet.io/c/en/' +\n",
    "                       concept + '?offset=' + str(offset) + '&limit=100').json()\n",
    "    all_edges = req\n",
    "    return all_edges\n",
    "\n",
    "\n",
    "def get_conc(data, path):\n",
    "    valid_relations = ['Synonym', 'RelatedTo', 'FormOf', 'IsA', 'PartOf', 'UsedFor', 'CapableOf',\n",
    "                       'Antonym', 'DefinedAs', 'SimilarTo', 'EtymologicallyRelatedTo',\n",
    "                       'ReceivesAction']\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    chunk = list(chunks(data, 500))\n",
    "    i = 0\n",
    "    for ch in chunk:\n",
    "        train_conc_root = get_concepts_for_roots(ch)\n",
    "        with open(f'./{path}/{i}.json', 'w') as f:\n",
    "            non_null = [x for x in train_conc_root if x]\n",
    "            filtered = [x for conc in non_null for x in conc if x['rel']['label'] in valid_relations\n",
    "                        and x['start']['language'] == 'en' and x['end']['language'] == 'en']\n",
    "            json.dump(filtered, f)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "def merge_concepts(dirs):\n",
    "    res = []\n",
    "    for d in dirs:\n",
    "        files = os.listdir(d)\n",
    "        for file in files:\n",
    "            with open(os.path.join(d, file)) as f:\n",
    "                cont = json.load(f)\n",
    "                res += cont\n",
    "    return res\n",
    "\n",
    "\n",
    "def normalize_concepts(concepts):\n",
    "    res = []\n",
    "    for concept in concepts:\n",
    "        if concept['start']['language'] == 'en':\n",
    "            res.append({concept['start']['label'].lower(): concept})\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_concepts_local(word):\n",
    "    all_concepts = merge_concepts(['train_conc', 'dev_conc', 'test_conc'])\n",
    "    normalized = normalize_concepts(all_concepts)\n",
    "    return [v for dic in normalized for k, v in dic.items() if k == word]\n",
    "\n",
    "\n",
    "# with open('./all_concepts.json', 'w') as f:\n",
    "#     all_concepts = merge_concepts(['train_conc', 'dev_conc', 'test_conc'])\n",
    "#     normalized = normalize_concepts(all_concepts)\n",
    "#     json.dump(normalized, f)\n",
    "\n",
    "\n",
    "def get_rels(word):\n",
    "    concepts = get_concepts_local(word)\n",
    "\n",
    "    synonyms = []\n",
    "    related = []\n",
    "    forms = []\n",
    "    hyponyms = []\n",
    "    meronyms = []\n",
    "    holonyms = []\n",
    "    capabilities = []\n",
    "    causes = []\n",
    "    antonyms = []\n",
    "    meanings = []\n",
    "    similarities = []\n",
    "    common_origins = []\n",
    "    can_be_done_to = []\n",
    "\n",
    "    def _check_rel(rel_type, rel_list):\n",
    "        if concept['rel']['label'] == rel_type:\n",
    "            lab = concept['end']['label']\n",
    "            if lab not in rel_list:\n",
    "                rel_list.append(lab)\n",
    "\n",
    "    for concept in concepts:\n",
    "        _check_rel('Synonym', synonyms)\n",
    "        _check_rel('RelatedTo', related)\n",
    "        _check_rel('FormOf', forms)\n",
    "        _check_rel('IsA', hyponyms)\n",
    "        _check_rel('PartOf', meronyms)\n",
    "        _check_rel('UsedFor', holonyms)\n",
    "        _check_rel('CapableOf', capabilities)\n",
    "        _check_rel('Antonym', antonyms)\n",
    "        _check_rel('DefinedAs', meanings)\n",
    "        _check_rel('SimilarTo', similarities)\n",
    "        _check_rel('EtymologicallyRelatedTo', common_origins)\n",
    "        _check_rel('ReceivesAction', can_be_done_to)\n",
    "\n",
    "    return {\n",
    "        'synonyms': synonyms,\n",
    "        'related': related,\n",
    "        'forms': forms,\n",
    "        'hyponyms': hyponyms,\n",
    "        'meronyms': meronyms,\n",
    "        'holonyms': holonyms,\n",
    "        'capabilities': capabilities,\n",
    "        'antonyms': antonyms,\n",
    "        'meanings': meanings,\n",
    "        'similarities': similarities,\n",
    "        'common_origins': common_origins,\n",
    "        'can_be_done_to': can_be_done_to,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1902,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Feature extractors '''\n",
    "\n",
    "\n",
    "def feature_extractor_base(doc1, doc2):\n",
    "    feats = {}\n",
    "    feats['similarity'] = doc1.similarity(doc2)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_ner(doc1, doc2):\n",
    "    def _inner(doc):\n",
    "        return [x.ent_type_ for x in doc]\n",
    "    feats = {}\n",
    "\n",
    "    feats['ner'] = get_intersection(_inner(doc1), _inner(doc2))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_word(doc1, doc2):\n",
    "    def _lemm(doc):\n",
    "        return [x.lemma_ for x in doc]\n",
    "\n",
    "    def _noun(doc):\n",
    "        return [x.lemma_ for x in doc if x.pos_ == 'NOUN']\n",
    "\n",
    "    def _verb(doc):\n",
    "        return [x.lemma_ for x in doc if x.pos_ == 'VERB']\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['lemma'] = get_intersection(_lemm(doc1), _lemm(doc2))\n",
    "    feats['noun'] = get_intersection(_noun(doc1), _noun(doc2))\n",
    "    feats['verb'] = get_intersection(_verb(doc1), _verb(doc2))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_spacy_sim(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['similar'] = get_tokens_similarity(doc1, doc2)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_ngrams(doc1, doc2):\n",
    "    def _ng_lemma(doc, n):\n",
    "        if n == 3:\n",
    "            return [(x.lemma_, y.lemma_, z.lemma_) for (x, y, z) in (x for x in ngrams(doc, n))]\n",
    "        else:\n",
    "            return [(x.lemma_, y.lemma_) for (x, y) in (x for x in ngrams(doc, n))]\n",
    "\n",
    "    def _ng_text(doc, n):\n",
    "        if n == 3:\n",
    "            return [(x.text, y.text, z.text) for (x, y, z) in (x for x in ngrams(doc, n))]\n",
    "        else:\n",
    "            return [(x.text, y.text) for (x, y) in (x for x in ngrams(doc, n))]\n",
    "\n",
    "    def _ng_pos(doc, n):\n",
    "        if n == 3:\n",
    "            return [(x.pos_, y.pos_, z.pos_) for (x, y, z) in (x for x in ngrams(doc, n))]\n",
    "        else:\n",
    "            return [(x.pos_, y.pos_) for (x, y) in (x for x in ngrams(doc, n))]\n",
    "\n",
    "    def _ng_dep(doc, n):\n",
    "        if n == 3:\n",
    "            return [(x.dep_, y.dep_, z.dep_) for (x, y, z) in (x for x in ngrams(doc, n))]\n",
    "        else:\n",
    "            return [(x.dep_, y.dep_) for (x, y) in (x for x in ngrams(doc, n))]\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['ngr-2-pos'] = get_intersection(_ng_pos(doc1, 2), _ng_pos(doc2, 2))\n",
    "    feats['ngr-2-dep'] = get_intersection(_ng_dep(doc1, 2), _ng_dep(doc2, 2))\n",
    "    feats['ngr-2-lemma'] = get_intersection(\n",
    "        _ng_lemma(doc1, 2), _ng_lemma(doc2, 2))\n",
    "    feats['ngr-2-text'] = get_intersection(\n",
    "        _ng_text(doc1, 2), _ng_text(doc2, 2))\n",
    "    feats['ngr-3-pos'] = get_intersection(_ng_pos(doc1, 3), _ng_pos(doc2, 3))\n",
    "    feats['ngr-3-dep'] = get_intersection(_ng_dep(doc1, 3), _ng_dep(doc2, 3))\n",
    "    feats['ngr-3-lemma'] = get_intersection(\n",
    "        _ng_lemma(doc1, 3), _ng_lemma(doc2, 3))\n",
    "    feats['ngr-3-text'] = get_intersection(\n",
    "        _ng_text(doc1, 3), _ng_text(doc2, 3))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_stemm(doc1, doc2):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    def _stem_v(doc):\n",
    "        return [stemmer.stem(x.text) for x in doc if x.pos_ == 'VERB']\n",
    "\n",
    "    def _stem_n(doc):\n",
    "        return [stemmer.stem(x.text) for x in doc if x.pos_ == 'NOUN']\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['stemm-v'] = get_intersection(_stem_v(doc1), _stem_v(doc2))\n",
    "    feats['stemm-n'] = get_intersection(_stem_n(doc1), _stem_n(doc2))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_neg(doc1, doc2):\n",
    "    def _get_neg(doc):\n",
    "        neg = ['not', 'n\\'t', 'neither', 'nor', 'never', 'none', 'nowhere']\n",
    "        neg_processed = []\n",
    "        neg_ind = 100500\n",
    "        for tok in doc:\n",
    "            if tok.lower_ in neg:\n",
    "                neg_ind = tok.i\n",
    "            elif tok.pos_ == 'PUNCT':\n",
    "                neg_ind = 100500\n",
    "                neg_processed.append(tok.lemma_)\n",
    "            elif tok.i > neg_ind:\n",
    "                neg_processed.append('NOT_' + tok.lemma_)\n",
    "            else:\n",
    "                neg_processed.append(tok.lemma_)\n",
    "        return neg_processed\n",
    "\n",
    "    feats = {}\n",
    "    feats['neg'] = get_intersection(_get_neg(doc1), _get_neg(doc2))\n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_deps(doc1, doc2):\n",
    "    def _inner_1(doc):\n",
    "        return [(x.dep, x.head.dep) for x in doc]\n",
    "    \n",
    "    def _inner_2(doc):\n",
    "        return [(x.dep, x.head.pos_) for x in doc]\n",
    "    \n",
    "    def _inner_3(doc):\n",
    "        return [(x.pos_, x.head.pos_) for x in doc]\n",
    "    \n",
    "    def _inner_4(doc):\n",
    "        return [(x.lemma_, x.head.lemma_) for x in doc]\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['head-dep'] = get_intersection(_inner_1(doc1), _inner_1(doc2))\n",
    "    feats['head-pos'] = get_intersection(_inner_2(doc1), _inner_2(doc2))\n",
    "    feats['dep-pos'] = get_intersection(_inner_3(doc1), _inner_3(doc2))\n",
    "    feats['dep-lemma'] = get_intersection(_inner_4(doc1), _inner_4(doc2))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_semant(cache):\n",
    "    def inner(doc1, doc2):\n",
    "        feats = {}\n",
    "\n",
    "        def _get_rels(doc):\n",
    "            for tok in doc:\n",
    "                if tok.dep_ == 'ROOT':\n",
    "                    if tok.lemma_ not in cache:\n",
    "                        rels = get_rels(tok.lemma_)\n",
    "                        cache[tok.lemma_] = rels\n",
    "                        return rels\n",
    "                    else:\n",
    "                        return cache[tok.lemma_]\n",
    "\n",
    "        rels1 = _get_rels(doc1)\n",
    "        rels2 = _get_rels(doc2)\n",
    "        root1_tok = [x for x in doc1 if x.dep_ == 'ROOT'][0]\n",
    "        root2_tok = [x for x in doc2 if x.dep_ == 'ROOT'][0]\n",
    "        root1 = root1_tok.lemma_\n",
    "        root2 = root2_tok.lemma_\n",
    "\n",
    "        neg = ['not', 'n\\'t', 'neither', 'nor', 'never', 'none', 'nowhere']\n",
    "\n",
    "        feats['syn'] = len([x for x in set(rels2['synonyms'])\n",
    "                            if x == root1 and doc2[root2_tok.i - 1].text not in neg]) + \\\n",
    "            len([x for x in set(rels1['synonyms'])\n",
    "                 if x == root2 and doc1[root1_tok.i - 1].text not in neg])\n",
    "        feats['mean'] = len([x for x in set(rels2['meanings'])\n",
    "                             if x == root1 or x in rels1['meanings']])\n",
    "        feats['sim'] = len(\n",
    "            [x for x in set(rels2['similarities']) if x == root1])\n",
    "        feats['form'] = len([x for x in set(rels2['forms'])\n",
    "                             if x == root1 or x in rels1['forms']])\n",
    "        feats['ant'] = len(\n",
    "            [x for x in set(rels2['antonyms']) if x in rels1['antonyms']])\n",
    "\n",
    "        return feats\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Reporting '''\n",
    "\n",
    "\n",
    "clf = get_classifier()\n",
    "\n",
    "\n",
    "def get_data(docs, raw_data, feature_extractor):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for i, doc_pair in enumerate(docs):\n",
    "        nlp1, nlp2 = doc_pair\n",
    "\n",
    "        features.append(feature_extractor(nlp1, nlp2))\n",
    "        labels.append(raw_data[i]['gold_label'])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def print_result(train_docs, test_docs, train_raw_data, test_raw_data, feature_extractor):\n",
    "    X_train, y_train = get_data(train_docs, train_raw_data, feature_extractor)\n",
    "    X_dev, y_dev = get_data(test_docs, test_raw_data, feature_extractor)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classification_report(y_dev, clf.predict(X_dev)))\n",
    "    \n",
    "    \n",
    "''' Optimization helpers '''\n",
    "\n",
    "\n",
    "def get_nlps(data):\n",
    "    docs = []\n",
    "    for i, sent in enumerate(data):\n",
    "        docs.append((nlp(sent['sentence1']), nlp(sent['sentence2'])))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = get_nlps(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_docs = get_nlps(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = get_nlps(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (just simply use sentence similarity fn from spacy ¯\\_(ツ)_/¯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 20 epochs took 8 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.51      0.46      3237\n",
      "   entailment       0.43      0.62      0.51      3368\n",
      "      neutral       0.35      0.11      0.17      3219\n",
      "\n",
      "     accuracy                           0.42      9824\n",
      "    macro avg       0.40      0.42      0.38      9824\n",
      " weighted avg       0.40      0.42      0.38      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With NER intersection (погіршення, викидаємо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 8 seconds\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.53      0.47      3237\n",
      "   entailment       0.42      0.68      0.52      3368\n",
      "      neutral       0.60      0.07      0.13      3219\n",
      "\n",
      "     accuracy                           0.43      9824\n",
      "    macro avg       0.48      0.43      0.37      9824\n",
      " weighted avg       0.48      0.43      0.38      9824\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_ner)\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With words intersection (покращення є)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.57      0.52      3237\n",
      "   entailment       0.55      0.66      0.60      3368\n",
      "      neutral       0.45      0.27      0.34      3219\n",
      "\n",
      "     accuracy                           0.50      9824\n",
      "    macro avg       0.49      0.50      0.49      9824\n",
      " weighted avg       0.49      0.50      0.49      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word)\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With ngrams (покращення невеличке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1900,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.55      0.52      3237\n",
      "   entailment       0.57      0.65      0.61      3368\n",
      "      neutral       0.45      0.33      0.38      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.51      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Намагалась опрацювати заперечення. Не вийшло :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1901,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.55      0.52      3237\n",
      "   entailment       0.57      0.65      0.61      3368\n",
      "      neutral       0.45      0.33      0.38      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.51      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_neg\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. With dependencies (схоже, я не зрозуміла, як їх використати, бо покращення мізерне)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1903,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.50      0.55      0.53      3237\n",
      "   entailment       0.57      0.64      0.60      3368\n",
      "      neutral       0.46      0.35      0.40      3219\n",
      "\n",
      "     accuracy                           0.52      9824\n",
      "    macro avg       0.51      0.51      0.51      9824\n",
      " weighted avg       0.51      0.52      0.51      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. With stemms (не допомогло, тому викидаємо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1904,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.50      0.55      0.53      3237\n",
      "   entailment       0.57      0.64      0.60      3368\n",
      "      neutral       0.46      0.35      0.40      3219\n",
      "\n",
      "     accuracy                           0.52      9824\n",
      "    macro avg       0.51      0.52      0.51      9824\n",
      " weighted avg       0.51      0.52      0.51      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps,\n",
    "                            feature_extractor_stemm\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. With semantic relations (моє найбільше розчарування...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1905,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.50      0.55      0.53      3237\n",
      "   entailment       0.57      0.65      0.60      3368\n",
      "      neutral       0.46      0.35      0.40      3219\n",
      "\n",
      "     accuracy                           0.52      9824\n",
      "    macro avg       0.51      0.52      0.51      9824\n",
      " weighted avg       0.51      0.52      0.51      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps,\n",
    "                            feature_extractor_semant({})\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. With spacy similarity (остання надія - на штучний інтелект, раз бракує свого :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1906,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.51      0.55      0.53      3237\n",
      "   entailment       0.58      0.63      0.60      3368\n",
      "      neutral       0.47      0.38      0.42      3219\n",
      "\n",
      "     accuracy                           0.52      9824\n",
      "    macro avg       0.52      0.52      0.52      9824\n",
      " weighted avg       0.52      0.52      0.52      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps,\n",
    "                            feature_extractor_spacy_sim,\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
