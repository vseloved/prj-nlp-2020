{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project metrics explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good source of all NLP metrics: https://github.com/gcunhase/NLPMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2(3) parts of my course project:\n",
    "\n",
    "* grapheme-to-phoneme\n",
    "* speech-to-text\n",
    "* (questin answering as a bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### speech2text metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular metric for evaluating s2t models is WER.\n",
    "But we will also review here other metrics:  \n",
    "ROUGE, BLEU, METEOR and WAcc(as extension of WER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://en.wikipedia.org/wiki/Word_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from jiwer import wer\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def word_error_rate(r, h, split=True):\n",
    "    \"\"\"\n",
    "    Given two list of strings how many word error rate(insert, delete or substitution).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : str\n",
    "        reference sentence\n",
    "    H : str\n",
    "        hypothesis sentence\n",
    "    split : bool, default True\n",
    "        split sentence by words. In case of character error rate CER should be set as False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : float\n",
    "    \"\"\"\n",
    "    \n",
    "    if split:\n",
    "        r = r.split()\n",
    "        h = h.split()\n",
    "        \n",
    "    d = np.zeros((len(r) + 1) * (len(h) + 1), dtype=np.uint16)\n",
    "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
    "    for i in range(len(r) + 1):\n",
    "        for j in range(len(h) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    for i in range(1, len(r) + 1):\n",
    "        for j in range(1, len(h) + 1):\n",
    "            if r[i - 1] == h[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    result = float(d[len(r)][len(h)]) / len(r)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = \"Це тестове синтезоване речення яке порівнюється з синтезованим моделлю іншим\"\n",
    "reference = \"Це тестове еталонне речення яке ми будемо намагатися порівняти з синтезованим моделлю\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_error_rate(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(truth=reference, hypothesis=hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### real-life example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('stt_report.xlsx')\n",
    "df['transcript'] = df['transcript'].map(lambda x: re.sub(r\"[\\.\\?\\,]\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 644 ms, sys: 635 µs, total: 645 ms\n",
      "Wall time: 644 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df['wer'] = df.apply(lambda x: wer(truth=x.transcript, hypothesis=x.pred_gc), axis=1)\n",
    "df['wer'] = np.where(df['wer'] > 1, 1, df['wer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>transcript</th>\n",
       "      <th>pred_gc</th>\n",
       "      <th>wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PILOT_20200206-201536_VDAD_0797068313_1239978-...</td>\n",
       "      <td>alo cho hỏi bùi thái an đang nghe máy đúng khô...</td>\n",
       "      <td>alo phải không chị thấy em luôn hả chị</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PILOT_20200206-201537_VDAD_0915234827_1239965-...</td>\n",
       "      <td>cảm ơn đã chờ máy chị gấm nghe máy hả chị Alo ...</td>\n",
       "      <td>cảm ơn đã kiểu bánh chị dám nghe máy hả chị alo</td>\n",
       "      <td>0.918182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PILOT_20200206-201551_VDAD_0777060852_1239985-...</td>\n",
       "      <td>cảm ơn đã chờ máy</td>\n",
       "      <td>cảm ơn đã kiểu bánh</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PILOT_20200206-201800_VDAD_0345591318_1240055-...</td>\n",
       "      <td>alo cho em hỏi anh thế nghe máy hả anh Dạ em c...</td>\n",
       "      <td>alo cho em hỏi anh thế mà máy em chào anh thì ...</td>\n",
       "      <td>0.971208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PILOT_20200206-202015_VDAD_0936646364_1240105-...</td>\n",
       "      <td>alo cho hỏi số thuê bao này của vũ thị hằng đú...</td>\n",
       "      <td>zalo cho thuê bao này của vũ thị hằng</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PILOT_20200206-202142_VDAD_0979028682_1240142-...</td>\n",
       "      <td>alo cho hỏi số thuê bao này của trương thị loa...</td>\n",
       "      <td>hello của trương thị lan phú yên ai là trương ...</td>\n",
       "      <td>0.748148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PILOT_20200206-202202_VDAD_0907890030_1240170-...</td>\n",
       "      <td>alo chào anh cho hỏi chị phải số điện thoại củ...</td>\n",
       "      <td>alo chào anh cho hỏi cái điện thoại này có chị...</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PILOT_20200206-202313_VDAD_0394408951_1240218-...</td>\n",
       "      <td>alo cho hỏi phải anh phước đang nghe máy không...</td>\n",
       "      <td>alo cho hỏi phan phước đang nghe máy không mà ...</td>\n",
       "      <td>0.729730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PILOT_20200206-202316_VDAD_0947228560_1240217-...</td>\n",
       "      <td>alo</td>\n",
       "      <td>alo</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  PILOT_20200206-201536_VDAD_0797068313_1239978-...   \n",
       "1  PILOT_20200206-201537_VDAD_0915234827_1239965-...   \n",
       "2  PILOT_20200206-201551_VDAD_0777060852_1239985-...   \n",
       "3  PILOT_20200206-201800_VDAD_0345591318_1240055-...   \n",
       "4  PILOT_20200206-202015_VDAD_0936646364_1240105-...   \n",
       "5  PILOT_20200206-202142_VDAD_0979028682_1240142-...   \n",
       "6  PILOT_20200206-202202_VDAD_0907890030_1240170-...   \n",
       "7  PILOT_20200206-202313_VDAD_0394408951_1240218-...   \n",
       "8  PILOT_20200206-202316_VDAD_0947228560_1240217-...   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  alo cho hỏi bùi thái an đang nghe máy đúng khô...   \n",
       "1  cảm ơn đã chờ máy chị gấm nghe máy hả chị Alo ...   \n",
       "2                                  cảm ơn đã chờ máy   \n",
       "3  alo cho em hỏi anh thế nghe máy hả anh Dạ em c...   \n",
       "4  alo cho hỏi số thuê bao này của vũ thị hằng đú...   \n",
       "5  alo cho hỏi số thuê bao này của trương thị loa...   \n",
       "6  alo chào anh cho hỏi chị phải số điện thoại củ...   \n",
       "7  alo cho hỏi phải anh phước đang nghe máy không...   \n",
       "8                                                alo   \n",
       "\n",
       "                                             pred_gc       wer  \n",
       "0             alo phải không chị thấy em luôn hả chị  0.730769  \n",
       "1    cảm ơn đã kiểu bánh chị dám nghe máy hả chị alo  0.918182  \n",
       "2                                cảm ơn đã kiểu bánh  0.400000  \n",
       "3  alo cho em hỏi anh thế mà máy em chào anh thì ...  0.971208  \n",
       "4              zalo cho thuê bao này của vũ thị hằng  0.428571  \n",
       "5  hello của trương thị lan phú yên ai là trương ...  0.748148  \n",
       "6  alo chào anh cho hỏi cái điện thoại này có chị...  0.437500  \n",
       "7  alo cho hỏi phan phước đang nghe máy không mà ...  0.729730  \n",
       "8                                                alo  0.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref, hyp = df.iloc[6].values[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(truth=ref, hypothesis=hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.596012024507655"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df['wer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grapheme2phoneme metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation approach is similar to s2t, but instead of words we compare phonemes. Equation for such calculation is similar to WER, so we can use the same function for grapheme error rate evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = \"Д' і в ч и н а\"\n",
    "word2 = 'Д и в ч і н а'\n",
    "word3 = \"Н а к р у т и л а с'\"\n",
    "word4 = \"Н а т р у т и л а с' я\"\n",
    "\n",
    "print(word_error_rate(word1, word2))\n",
    "word_error_rate(word3, word4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reporting the performance of a speech recognition system, sometimes word accuracy (WAcc) is used instead\n",
    "\n",
    "WAcc = 1 - WER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all metrics below are going to be maximized, we will use WAcc instead of WER for comparing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing other NLP metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.aclweb.org/anthology/W04-1013.pdf  \n",
    "\n",
    "human explanation: https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would explaing ROUGE as text summarization based precision/recal/f1_score metric.\n",
    "It takes into account unigrams, bigrams of tested texts, aswell longest common sequence or common skip-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Це тестове еталонне речення яке ми будемо намагатися порівняти з синтезованим моделлю\n",
      "Це тестове синтезоване речення яке порівнюється з синтезованим моделлю іншим\n"
     ]
    }
   ],
   "source": [
    "print(reference)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.6363636314049588, 'p': 0.7, 'r': 0.5833333333333334},\n",
       "  'rouge-2': {'f': 0.39999999505,\n",
       "   'p': 0.4444444444444444,\n",
       "   'r': 0.36363636363636365},\n",
       "  'rouge-l': {'f': 0.6363636314049588, 'p': 0.7, 'r': 0.5833333333333334}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps=hypothesis, refs=reference)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"WAcc:\", 1 - wer(truth=reference, hypothesis=hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alo chào anh cho hỏi chị phải số điện thoại của chị kim ngân không ạ\n",
      "alo chào anh cho hỏi cái điện thoại này có chị không ạ\n"
     ]
    }
   ],
   "source": [
    "print(ref)\n",
    "print(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.6896551674673009, 'p': 0.7692307692307693, 'r': 0.625},\n",
       "  'rouge-2': {'f': 0.4444444395061729, 'p': 0.5, 'r': 0.4},\n",
       "  'rouge-l': {'f': 0.7142857093112245,\n",
       "   'p': 0.7692307692307693,\n",
       "   'r': 0.6666666666666666}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps=hyp, refs=ref)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc: 0.5625\n"
     ]
    }
   ],
   "source": [
    "print(\"WAcc:\", 1 - wer(truth=ref, hypothesis=hyp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the main problem of applying ROUGE to s2t problems is that in takes into account only TP and FP samples, but mostly ignores the order of words as well as distance.  \n",
    "Of course, somehows rouge-N will take it into account with bigger N-grams. But it's not as clear as levenstein approach in WER.  \n",
    "ROUGE-L also not really applicable, as we only count common longest sequence and ignores all other staff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in general, this metric can be used fot s2t. IMO, it's just not as good as WER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.aclweb.org/anthology/P02-1040.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From article:\n",
    "\n",
    "BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations.\n",
    "\n",
    "The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Це тестове еталонне речення яке ми будемо намагатися порівняти з синтезованим моделлю\n",
      "Це тестове синтезоване речення яке порівнюється з синтезованим моделлю іншим\n"
     ]
    }
   ],
   "source": [
    "print(reference)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"WAcc:\", 1 - wer(truth=reference, hypothesis=hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE 1-gram: 0.5731115271545874\n",
      "BLUE 2-gram: 0.3638803347013253\n",
      "BLUE cummulative 2-gram: 0.4566661957296586\n",
      "BLUE cummulative 3-gram: 0.2804035645280751\n",
      "BLUE cummulative 4-gram (default): 4.440517594603186e-78\n"
     ]
    }
   ],
   "source": [
    "print(\"BLUE 1-gram:\", sentence_bleu(references=[reference.split()], hypothesis=hypothesis.split(), weights=(1, 0, 0, 0)))\n",
    "print(\"BLUE 2-gram:\", sentence_bleu(references=[reference.split()], hypothesis=hypothesis.split(), weights=(0, 1, 0, 0)))\n",
    "print(\"BLUE cummulative 2-gram:\", sentence_bleu(references=[reference.split()], hypothesis=hypothesis.split(), weights=(0.5, 0.5, 0, 0)))\n",
    "print(\"BLUE cummulative 3-gram:\", sentence_bleu(references=[reference.split()], hypothesis=hypothesis.split(), weights=(0.33, 0.33, 0.33, 0)))\n",
    "print(\"BLUE cummulative 4-gram (default):\", sentence_bleu(references=[reference.split()], hypothesis=hypothesis.split(), weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alo chào anh cho hỏi chị phải số điện thoại của chị kim ngân không ạ\n",
      "alo chào anh cho hỏi cái điện thoại này có chị không ạ\n"
     ]
    }
   ],
   "source": [
    "print(ref)\n",
    "print(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc: 0.5625\n"
     ]
    }
   ],
   "source": [
    "print(\"WAcc:\", 1 - wer(truth=ref, hypothesis=hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE 1-gram: 0.6107097367830394\n",
      "BLUE 2-gram: 0.3969613289089756\n",
      "BLUE cummulative 2-gram: 0.4923699307340427\n",
      "BLUE cummulative 3-gram: 0.37724841138205684\n",
      "BLUE cummulative 4-gram (default): 0.30215132342213097\n"
     ]
    }
   ],
   "source": [
    "print(\"BLUE 1-gram:\", sentence_bleu(references=[ref.split()], hypothesis=hyp.split(), weights=(1, 0, 0, 0)))\n",
    "print(\"BLUE 2-gram:\", sentence_bleu(references=[ref.split()], hypothesis=hyp.split(), weights=(0, 1, 0, 0)))\n",
    "print(\"BLUE cummulative 2-gram:\", sentence_bleu(references=[ref.split()], hypothesis=hyp.split(), weights=(0.5, 0.5, 0, 0)))\n",
    "print(\"BLUE cummulative 3-gram:\", sentence_bleu(references=[ref.split()], hypothesis=hyp.split(), weights=(0.33, 0.33, 0.33, 0)))\n",
    "print(\"BLUE cummulative 4-gram (default):\", sentence_bleu(references=[ref.split()], hypothesis=hyp.split(), weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the same situation as with ROUGE, we are counting N-grams. \n",
    "And if in case of ROUGE-S, we can at least capture such case as:  \n",
    "\"мама мыла раму\" -> \"мама раму\"  \n",
    "by using skip-grams, both metrics will ignore such sample:  \n",
    "\"мама мыла раму\" -> \"раму мама мыла\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "0.6065306597126334\n",
      "9.047424648113057e-155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.7999999952000001, 'p': 1.0, 'r': 0.6666666666666666},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.7999999952000001, 'p': 1.0, 'r': 0.6666666666666666}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(1- wer(\"мама мыла раму\", \"мама раму\"))\n",
    "print(sentence_bleu(references=[\"мама мыла раму\".split()], hypothesis=\"мама раму\".split(), weights=(1, 0, 0, 0)))\n",
    "print(sentence_bleu(references=[\"мама мыла раму\".split()], hypothesis=\"мама раму\".split(), weights=(0.5, 0.5, 0, 0)))\n",
    "rouge.get_scores(hyps=\"мама раму\", refs=\"мама мыла раму\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333333333333337\n",
      "1.0\n",
      "0.7071067811865476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.4999999950000001, 'p': 0.5, 'r': 0.5},\n",
       "  'rouge-l': {'f': 0.6666666616666668,\n",
       "   'p': 0.6666666666666666,\n",
       "   'r': 0.6666666666666666}}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(1- wer(\"мама мыла раму\", \"раму мама мыла\"))\n",
    "print(sentence_bleu(references=[\"мама мыла раму\".split()], hypothesis=\"раму мама мыла\".split(), weights=(1, 0, 0, 0)))\n",
    "print(sentence_bleu(references=[\"мама мыла раму\".split()], hypothesis=\"раму мама мыла\".split(), weights=(0.5, 0.5, 0, 0)))\n",
    "rouge.get_scores(hyps=\"раму мама мыла\", refs=\"мама мыла раму\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this is important? For some languages, like Enlgish, analytical languages, the order of words make sense.\n",
    "And only WER penalize us for changing predicted words order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tl;dr\n",
    "METEOR is BLUE on steroids. In addition to comparing N-grams, it also takes into account coincidences of stemmed words and synonyms. This ideally lays on MT problem, but mostly useless for s2t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Це тестове еталонне речення яке ми будемо намагатися порівняти з синтезованим моделлю\n",
      "Це тестове синтезоване речення яке порівнюється з синтезованим моделлю іншим\n"
     ]
    }
   ],
   "source": [
    "print(reference)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"WAcc:\", 1 - wer(truth=reference, hypothesis=hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5698720166032515"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_meteor_score(reference=reference, hypothesis=hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alo chào anh cho hỏi chị phải số điện thoại của chị kim ngân không ạ\n",
      "alo chào anh cho hỏi cái điện thoại này có chị không ạ\n"
     ]
    }
   ],
   "source": [
    "print(ref)\n",
    "print(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc: 0.5625\n"
     ]
    }
   ],
   "source": [
    "print(\"WAcc:\", 1 - wer(truth=ref, hypothesis=hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6165605095541401"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_meteor_score(reference=ref, hypothesis=hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, all 3 metrics are works with N-grams.  \n",
    "``ROUGE-S`` is good, because takes into account skip-grams, but on the other hand, it ignores order. And fact that it's implementation is absent in all common used libraries, makes me think that it actually not popular among researchers.  \n",
    "``BLUE`` just comparing common n-grams and aggregates results.  \n",
    "``METEOR`` is the best one for this purposes, as it also aligns hypothesis to the reference, which affects on ``penalty`` in the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, h = \"мама мыла раму\", \"мама раму\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc:  0.6666666666666667\n",
      "BLUE 1-gram:  0.6065306597126334\n",
      "BLUE commulative 2-gram:  9.047424648113057e-155\n",
      "METEOR:  0.3448275862068965\n",
      "ROUGE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.7999999952000001, 'p': 1.0, 'r': 0.6666666666666666},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.7999999952000001, 'p': 1.0, 'r': 0.6666666666666666}}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"WAcc: \", 1- wer(r, h))\n",
    "print(\"BLUE 1-gram: \", sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(1, 0, 0, 0)))\n",
    "print(\"BLUE commulative 2-gram: \", sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(0.5, 0.5, 0, 0)))\n",
    "print(\"METEOR: \", single_meteor_score(reference=r, hypothesis=h))\n",
    "print(\"ROUGE:\")\n",
    "rouge.get_scores(hyps=h, refs=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, h = \"мама мыла раму\", \"раму мыла мама\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc:  0.33333333333333337\n",
      "BLUE 1-gram:  1.0\n",
      "BLUE commulative 2-gram:  1.491668146240062e-154\n",
      "METEOR:  0.5\n",
      "ROUGE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.3333333283333334,\n",
       "   'p': 0.3333333333333333,\n",
       "   'r': 0.3333333333333333}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"WAcc: \", 1- wer(r, h))\n",
    "print(\"BLUE 1-gram: \", sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(1, 0, 0, 0)))\n",
    "print(\"BLUE commulative 2-gram: \", sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(0.5, 0.5, 0, 0)))\n",
    "print(\"METEOR: \", single_meteor_score(reference=r, hypothesis=h))\n",
    "print(\"ROUGE:\")\n",
    "rouge.get_scores(hyps=h, refs=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, h = \"the cat sat on the mat\", \"sat on the mat the cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc:  0.33333333333333337\n",
      "BLUE 1-gram:  1.0\n",
      "BLUE commulative 2-gram:  0.8944271909999159\n",
      "METEOR:  0.7106481481481481\n",
      "ROUGE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.999999995, 'p': 1.0, 'r': 1.0},\n",
       "  'rouge-2': {'f': 0.7999999950000002, 'p': 0.8, 'r': 0.8},\n",
       "  'rouge-l': {'f': 0.7999999950000002, 'p': 0.8, 'r': 0.8}}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"WAcc: \", 1- wer(r, h))\n",
    "print(\"BLUE 1-gram: \", sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(1, 0, 0, 0)))\n",
    "print(\"BLUE commulative 2-gram: \", sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(0.5, 0.5, 0, 0)))\n",
    "print(\"METEOR: \", single_meteor_score(reference=r, hypothesis=h))\n",
    "print(\"ROUGE:\")\n",
    "rouge.get_scores(hyps=h, refs=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAcc exec time: 0.88 miliseconds\n",
      "WAcc with JIT exec time: 0.64 miliseconds\n",
      "BLUE 1-gram exec time: 0.56 miliseconds\n",
      "BLUE commulative 2-gram exec time: 0.50 miliseconds\n",
      "METEOR exec time: 0.24 miliseconds\n",
      "ROUGE exec time: 0.56 miliseconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "_ = 1- wer(r, h)\n",
    "print(f\"WAcc exec time: {(time() - start_time) * 1000:.2f} miliseconds\")\n",
    "\n",
    "start_time = time()\n",
    "_ = 1- word_error_rate(r, h)\n",
    "print(f\"WAcc with JIT exec time: {(time() - start_time) * 1000:.2f} miliseconds\")\n",
    "\n",
    "start_time = time()\n",
    "_ = sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(1, 0, 0, 0))\n",
    "print(f\"BLUE 1-gram exec time: {(time() - start_time) * 1000:.2f} miliseconds\")\n",
    "\n",
    "start_time = time()\n",
    "_ = sentence_bleu(references=[r.split()], hypothesis=h.split(), weights=(0.5, 0.5, 0, 0))\n",
    "print(f\"BLUE commulative 2-gram exec time: {(time() - start_time) * 1000:.2f} miliseconds\")\n",
    "\n",
    "start_time = time()\n",
    "_ = single_meteor_score(reference=r, hypothesis=h)\n",
    "print(f\"METEOR exec time: {(time() - start_time) * 1000:.2f} miliseconds\")\n",
    "\n",
    "start_time = time()\n",
    "_ = rouge.get_scores(hyps=h, refs=r)\n",
    "print(f\"ROUGE exec time: {(time() - start_time) * 1000:.2f} miliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are no performance advantage among described metrics. So, this can't be a point to chose the best one.\n",
    "Howewer, if the word order in generated text is important for us, we should choose ``METEOR`` or ``WAcc``.\n",
    "``METEOR`` more penalizing absent words, ``WAcc`` is just ``Levenstain distance`` for sentences and takes everything into account evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p36] *",
   "language": "python",
   "name": "conda-env-p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
