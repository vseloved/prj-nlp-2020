{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "TASK_PATH = os.path.join(REPO_PATH, \"tasks\", \"02-structural-linguistics\")\n",
    "DATA_PATH = os.path.join(TASK_PATH, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_markdown(path):\n",
    "    with open(path, 'r') as md:\n",
    "        content = md.read()\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Заголовки новин\n",
       "\n",
       "### 1. Форматування\n",
       "\n",
       "[The Associated Press Stylebook](https://www.amazon.com/Associated-Press-Stylebook-2017-Briefing/dp/0465093043/) - це посібник зі стилю, яким часто послуговуються журналісти по всьому світу. Він рекомендує такі правила форматування заголовків:\n",
       "1. З великої літери потрібно писати слова довжиною 4 чи більше літер.\n",
       "2. З великої літери потрібно писати перше і останнє слово заголовку, незалежно від частини мови.\n",
       "3. З великої літери потрібно писати іменники, займенники, дієслова, прикметники, прислівники та підрядні сполучники.\n",
       "4. Якщо слово написане через дефіс, велику літеру потрібно додати для кожної частинки слова (наприклад, правильно \"Self-Reflection\", а не \"Self-reflection\").\n",
       "5. З маленької літери потрібно писати всі інші частини мови: артиклі/визначники, сурядні сполучники, прийменники, частки, вигуки.\n",
       "\n",
       "**Завдання:**\n",
       "1. напишіть програму, яка форматує заголовки за вказаними правилами\n",
       "2. перевірте якість роботи програми на [валідаційній вибірці](data/headlines-test-set.json)\n",
       "3. проженіть вашу програму на [корпусі заголовків з The Examiner](data/examiner-headlines.txt) і вирахуйте, скільки заголовків там відформатовано за правилами (скільки заголовків залишились незмінними після форматування)\n",
       "4. збережіть програму та числові результати у директорії з вашим іменем\n",
       "\n",
       "Якщо потрібно продебажити роботу програми, робіть це на [корпусі заголовків з The Examiner](data/examiner-headlines.txt), а не на валідаційній вибірці. Спробуйте досягти хоча б 90% якості на валідаційній вибірці. Якість рахуємо за повним збігом відформатованого заголовка.\n",
       "\n",
       "Підказка: ваша програма повинна правильно розрізняти прийменники та підрядні сполучники. Наприклад, `Do as you want` => `Do As You Want` (бо \"as\" тут є сполучником), but `How to use a Macbook as a table` => `How to Use a Macbook as a Table` (бо \"as\" тут є прийменником).\n",
       "\n",
       "### 2. Вірусні новини\n",
       "\n",
       "У статті [Automatic Extraction of News Values from Headline Text](http://www.aclweb.org/anthology/E17-4007) описано основні ознаки заголовків, які кидаються в очі і змушують читача таки прочитати новину:\n",
       "1. іменовані сутності (імена людей, назви компаній тощо)\n",
       "2. емоційне забарвлення\n",
       "3. вищий і найвищий ступені порівняння\n",
       "4. близькість до читача\n",
       "5. елемент несподіванки\n",
       "6. унікальність\n",
       "\n",
       "**Завдання:**\n",
       "1. Напишіть програму, яка аналізує заголовок за першими трьома ознаками (у спрощеній формі)\n",
       "   * Чи є в заголовку іменовані стуності?\n",
       "   * Чи є заголовок позитивно чи негативно забарвлений?\n",
       "   * Чи є в заголовку прикметники та прислівники вищого і найвищого ступенів порівняння?\n",
       "2. Проженіть вашу програму на [корпусі заголовків з The Examiner](data/examiner-headlines.txt). Для кожної з трьох ознак визначте відсоток заголовків у корпусі, які її мають.\n",
       "3. Збережіть програму та пораховану статистику в директорії з вашим іменем.\n",
       "\n",
       "Додаткова інформація:\n",
       "- Типи сутностей, які впливають на \"вірусність\" заголовка, виберіть самостійно.\n",
       "- Для визначення емоційного забарвлення, використайте [SentiWordNet](https://github.com/aesuli/sentiwordnet). Наприклад, можна перевірити, що середнє значення позитивності/негативності слова у заголовку перевищує 0.5. Для визначення середнього значення можна брати до п'яти перших значень слова з такою частиною мови. Будьте креативними та експериментуйте. Можна користуватися SentiWordNet з бібліотеки [NLTK](http://www.nltk.org/howto/sentiwordnet.html).\n",
       "\n",
       "### Джерела\n",
       "\n",
       "Ви можете використати будь-яку мову програмування та будь-яку NLP-бібліотеку.\n",
       "\n",
       "Набір заголовків взятий із https://www.kaggle.com/therohk/examine-the-examiner.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_markdown(os.path.join(TASK_PATH, \"2-headlines.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Halep enters Rogers Cup final in straight sets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The phantoms of St. Mary's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Talladega turmoil could spell trouble for NASC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burn those calories! Try the Very Steep Trail.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's the end of the world... and I feel fine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original\n",
       "0  Halep enters Rogers Cup final in straight sets...\n",
       "1                         The phantoms of St. Mary's\n",
       "2  Talladega turmoil could spell trouble for NASC...\n",
       "3     Burn those calories! Try the Very Steep Trail.\n",
       "4       It's the end of the world... and I feel fine"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_table(os.path.join(DATA_PATH, \"examiner-headlines.txt\"), names=[\"original\"])\n",
    "\n",
    "print(val_df.shape)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How To Design A College Curriculum to Help You...</td>\n",
       "      <td>How to Design a College Curriculum to Help You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is why you should hate Battlefield 3</td>\n",
       "      <td>This Is Why You Should Hate Battlefield 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to photograph tonight's Lyrid Meteor Shower</td>\n",
       "      <td>How to Photograph Tonight's Lyrid Meteor Shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Teresa Giudice broke: 'RHONJ' star can't even ...</td>\n",
       "      <td>Teresa Giudice Broke: 'RHONJ' Star Can't Even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murder Mystery Dinner Theater - 'Who Killed Cu...</td>\n",
       "      <td>Murder Mystery Dinner Theater - 'Who Killed Cu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                true  \\\n",
       "0  How To Design A College Curriculum to Help You...   \n",
       "1          This is why you should hate Battlefield 3   \n",
       "2    How to photograph tonight's Lyrid Meteor Shower   \n",
       "3  Teresa Giudice broke: 'RHONJ' star can't even ...   \n",
       "4  Murder Mystery Dinner Theater - 'Who Killed Cu...   \n",
       "\n",
       "                                                test  \n",
       "0  How to Design a College Curriculum to Help You...  \n",
       "1          This Is Why You Should Hate Battlefield 3  \n",
       "2    How to Photograph Tonight's Lyrid Meteor Shower  \n",
       "3  Teresa Giudice Broke: 'RHONJ' Star Can't Even ...  \n",
       "4  Murder Mystery Dinner Theater - 'Who Killed Cu...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json(os.path.join(DATA_PATH, \"headlines-test-set.json\"))\n",
    "test_df.columns = [\"true\", \"test\"]\n",
    "\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Craigslist advises how to deliver; seller robbed following advice',\n",
       "        'Craigslist Advises How to Deliver; Seller Robbed Following Advice']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles = [\"a\", \"an\", \"the\"]\n",
    "# particles = [\"not\"]\n",
    "\n",
    "\n",
    "def format_title(text, prnt=False):\n",
    "    tokens = nlp(text)\n",
    "    new_tokens = [token.text for token in tokens]\n",
    "    \n",
    "    if prnt:\n",
    "        for item in [(token.text, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop) for token in tokens]:\n",
    "            print(item)\n",
    "    \n",
    "    res = []\n",
    "    skip = False\n",
    "    for index, token in enumerate(tokens):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "            \n",
    "#         if token.pos_ == \"PROPN\":\n",
    "#             if len(re.findall(r\"[A-Z]+\", token.text)) == 0:\n",
    "#                 new_tokens[index] = token.text.capitalize()\n",
    "#             else:\n",
    "#                 new_tokens[index] = token.text\n",
    "\n",
    "        if len(re.findall(r\"[A-Z]+\", token.text)) > 0:\n",
    "            new_tokens[index] = token.text\n",
    "            continue\n",
    "                \n",
    "        elif token.pos_ in [\"NOUN\", \"PRON\", \"VERB\", \"ADV\", \"ADJ\", \"NUM\"]:\n",
    "            new_tokens[index] = token.text.capitalize()\n",
    "            \n",
    "        elif token.pos_ == 'DET' and token.dep_ in (\"poss\", \"appos\"):\n",
    "            new_tokens[index] = token.text.capitalize()\n",
    "        \n",
    "        elif token.pos_ == 'SCONJ' and token.dep_ == \"mark\":\n",
    "            new_tokens[index] = token.text.capitalize()\n",
    "            \n",
    "#         elif token.pos_ == 'PART' and token.tag_ == 'RB' and token.dep_ == \"neg\":\n",
    "#             new_tokens[index] = token.text.capitalize()\n",
    "        elif token.text.lower() == 'not':\n",
    "            new_tokens[index] = token.text.capitalize()\n",
    "            \n",
    "        elif token.tag_.startswith(\"VB\"):\n",
    "            new_tokens[index] = token.text.capitalize()\n",
    "                \n",
    "        elif len(token.text) > 3:\n",
    "            new_tokens[index] = token.text.capitalize()\n",
    "            \n",
    "        elif token.text == \"-\":\n",
    "            new_tokens[index-1] = tokens[index-1].text.capitalize()\n",
    "            new_tokens[index+1] = tokens[index+1].text.capitalize()\n",
    "            skip = True\n",
    "            \n",
    "        else:\n",
    "            new_tokens[index] = token.text\n",
    "    \n",
    "    new_tokens = [token + whitespace for token, whitespace in zip(new_tokens, [token.whitespace_ for token in tokens])]\n",
    "    new_tokens = \"\".join(new_tokens)\n",
    "    \n",
    "    new_tokens = new_tokens.split(\" \")\n",
    "    new_tokens[0] = new_tokens[0].capitalize() if len(re.findall(r\"[A-Z]+\", new_tokens[0])) == 0 else new_tokens[0]\n",
    "    new_tokens[-1] = new_tokens[-1].capitalize() if len(re.findall(r\"[A-Z]+\", new_tokens[-1])) == 0 else new_tokens[-1]\n",
    "\n",
    "    return \" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting accuracy, 100: 0.96 \n",
      "\n",
      "CPU times: user 577 ms, sys: 90 µs, total: 577 ms\n",
      "Wall time: 576 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_df[\"pred\"] = test_df[\"true\"].map(format_title)\n",
    "print(\"Formatting accuracy, 100:\", test_df.loc[test_df[\"pred\"] == test_df[\"test\"]].shape[0] / test_df.shape[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted by rules, 5000: 0.1482 \n",
      "\n",
      "CPU times: user 26.5 s, sys: 16.2 ms, total: 26.5 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "val_df[\"formatted\"] = val_df[\"original\"].map(format_title)\n",
    "print(\"Formatted by rules, 5000:\", val_df.loc[val_df[\"original\"] == val_df[\"formatted\"]].shape[0] / val_df.shape[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\t Former Jets kicker Jay Feely signs with Arizona Cardinals\n",
      "Formatted:\t Former Jets Kicker Jay Feely Signs With Arizona Cardinals \n",
      "\n",
      "Original:\t Art in Denver: O'Keefe exhibit soon gone\n",
      "Formatted:\t Art in Denver: O'Keefe Exhibit Soon Gone \n",
      "\n",
      "Original:\t 'Dr. Horrible' update\n",
      "Formatted:\t 'Dr. Horrible' Update \n",
      "\n",
      "Original:\t Matt Lauer and NBC's 'Today' pay tribute to Elizabeth Edwards (video)\n",
      "Formatted:\t Matt Lauer and NBC's 'Today' Pay Tribute to Elizabeth Edwards (Video) \n",
      "\n",
      "Original:\t Network externalities in the MMO community.\n",
      "Formatted:\t Network Externalities in the MMO Community. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in val_df.sample(5).values:\n",
    "    print(f\"Original:\\t {item[0]}\")\n",
    "    print(f\"Formatted:\\t {format_title(item[0])}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viral News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_df = pd.read_csv(\"SentiWordNet_3.0.0.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_df['SynsetTermsListed'] = senti_df['SynsetTerms'].map(lambda x: re.findall(r\"([\\w\\-]+)#\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_map = {}\n",
    "\n",
    "for pos, words, poss, neg in senti_df[[\"POS\", \"SynsetTermsListed\", \"PosScore\", \"NegScore\"]].values:\n",
    "    for word in words:\n",
    "        senti_map[pos + \"_\" + word] = senti_map.get(pos + \"_\" + word, []) + [(poss, neg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_map = {k: tuple(np.mean(np.array(v), axis=0)) for k, v in senti_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mapping = {\"NOUN\": \"n\", \"PROPN\": \"n\", \"ADJ\": \"a\", \"ADV\": \"r\", \"VERB\": \"v\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_viral(title, swn=True):\n",
    "    doc = nlp(title)\n",
    "    \n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    if not swn:\n",
    "        sentiment = TextBlob(title).sentiment.polarity\n",
    "    else:\n",
    "        sentiment = []\n",
    "    comp_degrees = []\n",
    "    \n",
    "    for index, token in enumerate(doc):\n",
    "        if swn:\n",
    "            sentiment.append(senti_map.get(pos_mapping.get(token.pos_, \"\") + \"_\" + token.text, (0, 0)))\n",
    "        if token.pos_ in (\"ADJ\", \"ADV\"):\n",
    "            if token.text.endswith((\"er\", \"est\")):\n",
    "                comp_degrees.append(token.text)\n",
    "            elif doc[index-1].text in (\"more\", \"most\"):\n",
    "                comp_degrees.append(doc[index-1].text + doc[index-1].whitespace_ + token.text)\n",
    "            else:\n",
    "                continue\n",
    "    if swn:\n",
    "        pos, neg = tuple(np.mean(np.array(sentiment), axis=0))\n",
    "        sentiment = pos - neg\n",
    "    return entities, sentiment, comp_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 s, sys: 27 µs, total: 24.6 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = list(map(lambda x: check_viral(x), val_df.original.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of titles with NER: 78.24%\n",
      "\n",
      "Percentage of titles with positive sentiment: 37.54%\n",
      "Percentage of titles with negative sentiment: 22.58%\n",
      "Percentage of titles with neutral sentiment: 39.88%\n",
      "\n",
      "Percentage of titles with comparison degrees: 4.92%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of titles with NER: {len(list(filter(lambda x: len(x[0])> 0, res))) / len(res)*100:.2f}%\")\n",
    "print()\n",
    "print(f\"Percentage of titles with positive sentiment: {len(list(filter(lambda x: x[1] > 0, res))) / len(res)*100:.2f}%\")\n",
    "print(f\"Percentage of titles with negative sentiment: {len(list(filter(lambda x: x[1] < 0, res))) / len(res)*100:.2f}%\")\n",
    "print(f\"Percentage of titles with neutral sentiment: {len(list(filter(lambda x: x[1] == 0, res))) / len(res)*100:.2f}%\")\n",
    "print()\n",
    "print(f\"Percentage of titles with comparison degrees: {len(list(filter(lambda x: len(x[2])> 0, res))) / len(res)*100:.2f}%\")\n",
    "\n",
    "\n",
    "# len(list(filter(lambda x: len(x[0])> 0, res))) / len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
