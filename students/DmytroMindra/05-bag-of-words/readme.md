Бэйзлайн: Комментарии токенизированы по пробелам.

**Cross Validation:**
```
SGDClassifier [0.25244541 0.26890177 0.20061863 0.29559161 0.30876904]
SGDClassifier, average 0.26526529357045614
```

**Baseline SGDClassifier:**
```
              precision    recall  f1-score   support

           1       0.00      0.00      0.00         9
           2       0.00      0.00      0.00        13
           3       0.14      0.07      0.09        15
           4       0.33      0.18      0.23        68
           5       0.72      0.91      0.80       216

    accuracy                           0.65       321
   macro avg       0.24      0.23      0.23       321
weighted avg       0.56      0.65      0.59       321

```

Итерация 1: Комментарии лемматизированы и отфильтрована пунктуация

```
SGDClassifier [0.2302924  0.30006645 0.19738059 0.28398268 0.34274781]
SGDClassifier, average 0.2708939852952751
```

**Baseline SGDClassifier on lemmas:**
```
              precision    recall  f1-score   support

           1       0.11      0.11      0.11         9
           2       0.33      0.15      0.21        13
           3       0.17      0.07      0.10        15
           4       0.23      0.18      0.20        68
           5       0.73      0.83      0.78       216

    accuracy                           0.61       321
   macro avg       0.31      0.27      0.28       321
weighted avg       0.56      0.61      0.58       321
```


**Выводы:**
1. На моем наборе данных логистическая регрессия работает лучше наивного байесовского классификатора
2. Лемматизация не дала большого прироста качества модели на этом наборе данных
3. Очень удобно сохранять и загружать промежуточные результаты, чтобы не пересчитывать их каждый раз
4. Явно недостаточно данных, продолжаю парсить другие категории 
