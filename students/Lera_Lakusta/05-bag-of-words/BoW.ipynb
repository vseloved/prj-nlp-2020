{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import csv\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_name</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>dignity</th>\n",
       "      <th>is_from_buyer</th>\n",
       "      <th>product_name</th>\n",
       "      <th>mark</th>\n",
       "      <th>percent_dignity</th>\n",
       "      <th>replies_number</th>\n",
       "      <th>shortcomings</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Мінеральна вода</td>\n",
       "      <td>45534917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Упаковка минеральной лечебно-столовой сильнога...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Все класно, тільки довго везуть. Привезли чере...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Мінеральна вода</td>\n",
       "      <td>45503604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Упаковка минеральной лечебно-столовой сильнога...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Тара стекло или пэт-пластик?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Мінеральна вода</td>\n",
       "      <td>45412834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Упаковка минеральной лечебно-столовой сильнога...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Рекомендую</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Мінеральна вода</td>\n",
       "      <td>41870818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Упаковка минеральной лечебно-столовой сильнога...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Да водичка супер!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Мінеральна вода</td>\n",
       "      <td>41169471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Упаковка минеральной лечебно-столовой сильнога...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Подскажите это оригинальная вода ???</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category_name  comment_id dignity  is_from_buyer  \\\n",
       "0  Мінеральна вода    45534917     NaN           True   \n",
       "1  Мінеральна вода    45503604     NaN          False   \n",
       "2  Мінеральна вода    45412834     NaN           True   \n",
       "3  Мінеральна вода    41870818     NaN           True   \n",
       "4  Мінеральна вода    41169471     NaN           True   \n",
       "\n",
       "                                        product_name  mark  percent_dignity  \\\n",
       "0  Упаковка минеральной лечебно-столовой сильнога...   5.0                0   \n",
       "1  Упаковка минеральной лечебно-столовой сильнога...   0.0              100   \n",
       "2  Упаковка минеральной лечебно-столовой сильнога...   5.0                0   \n",
       "3  Упаковка минеральной лечебно-столовой сильнога...   5.0              100   \n",
       "4  Упаковка минеральной лечебно-столовой сильнога...   5.0              100   \n",
       "\n",
       "   replies_number shortcomings  \\\n",
       "0               0          NaN   \n",
       "1               0          NaN   \n",
       "2               1          NaN   \n",
       "3               1          NaN   \n",
       "4               1          NaN   \n",
       "\n",
       "                                                text  \n",
       "0  Все класно, тільки довго везуть. Привезли чере...  \n",
       "1                       Тара стекло или пэт-пластик?  \n",
       "2                                         Рекомендую  \n",
       "3                                  Да водичка супер!  \n",
       "4               Подскажите это оригинальная вода ???  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = pd.read_csv('comments.csv', usecols=[0,1,3,4,5,6,7,8,9,10])\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    24514\n",
       "4.0     5794\n",
       "3.0     2106\n",
       "2.0     1256\n",
       "1.0     1202\n",
       "Name: mark, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows with no marks\n",
    "df_comments = df_comments[df_comments.mark != 0.0]\n",
    "df_comments = df_comments[df_comments['mark'].notna()]\n",
    "# drop rows with no text\n",
    "df_comments = df_comments.dropna(subset=['text', 'dignity', 'shortcomings'], how='all').reset_index(drop=True)\n",
    "df_comments.drop_duplicates(inplace = True)\n",
    "df_comments['mark'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Detect language for every comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ru    27024\n",
       "uk     5314\n",
       "bg      500\n",
       "mk      386\n",
       "en        4\n",
       "it        2\n",
       "fr        1\n",
       "ca        1\n",
       "et        1\n",
       "de        1\n",
       "af        1\n",
       "ro        1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_lang (text):\n",
    "    if text and re.search(r'[А-я]+', text):\n",
    "        return detect(text)\n",
    "\n",
    "df_comments['lang'] = df_comments['text'].fillna('').apply(detect_lang)\n",
    "df_comments['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Leave comments in Ukrainian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    3626\n",
       "4.0     979\n",
       "3.0     350\n",
       "2.0     181\n",
       "1.0     178\n",
       "Name: mark, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = df_comments[df_comments.lang == 'uk'].reset_index(drop=True)\n",
    "df_comments['mark'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Add label to every comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    4605\n",
       "negative     709\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_label (label):\n",
    "    if label:\n",
    "        if 5 >= label >= 4:\n",
    "            return 'positive'\n",
    "        else: return 'negative'\n",
    "\n",
    "df_comments['label'] = df_comments['mark'].apply(choose_label)\n",
    "df_comments['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text (text):\n",
    "    return BeautifulSoup(text, 'lxml').get_text().lower()\n",
    "\n",
    "# make downsampling \n",
    "\n",
    "# Shuffle the Dataset.\n",
    "shuffled_df = df_comments.sample(frac=1,random_state=4)\n",
    "shuffled_df\n",
    "\n",
    "neg_comments = shuffled_df.loc[shuffled_df['label'] == 'negative']\n",
    "pos_comments = shuffled_df.loc[shuffled_df['label'] == 'positive'].sample(n=709,random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "normalized_df = pd.concat([pos_comments, neg_comments])\n",
    "normalized_df['clean_text'] = normalized_df['text'].apply(clean_text)\n",
    "normalized_df['tokens'] = normalized_df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(normalized_df['clean_text'], normalized_df['label'],\n",
    "                                                    test_size = 0.3, random_state = 42,\n",
    "                                                    stratify = normalized_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "test_vec = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc = LogisticRegression(solver='sag', max_iter = 1000, random_state=42)\n",
    "lrc.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.74      0.75       213\n",
      "    positive       0.75      0.77      0.76       213\n",
      "\n",
      "    accuracy                           0.75       426\n",
      "   macro avg       0.75      0.75      0.75       426\n",
      "weighted avg       0.75      0.75      0.75       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y = lrc.predict(test_vec)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improved decision with lemmas, bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas (doc):\n",
    "    doc_lemmas = []\n",
    "    for text in doc:\n",
    "        sent_lemmas = []\n",
    "        for token in word_tokenize(text):\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            sent_lemmas.append(lemma)\n",
    "        sent = \" \".join(sent_lemmas)\n",
    "        doc_lemmas.append(sent)\n",
    "    return doc_lemmas\n",
    "\n",
    "X_train_lemmas = get_lemmas(X_train)\n",
    "X_test_lemmas = get_lemmas(X_test)\n",
    "        \n",
    "vectorizer = CountVectorizer(analyzer = 'char', ngram_range = (1,5))\n",
    "\n",
    "train_vec = vectorizer.fit_transform(X_train_lemmas).toarray()\n",
    "\n",
    "test_vec = vectorizer.transform(X_test_lemmas).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.73      0.75       213\n",
      "    positive       0.75      0.79      0.77       213\n",
      "\n",
      "    accuracy                           0.76       426\n",
      "   macro avg       0.76      0.76      0.76       426\n",
      "weighted avg       0.76      0.76      0.76       426\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lrc = LogisticRegression(solver='sag', max_iter = 1000, random_state=42)\n",
    "lrc.fit(train_vec, y_train)\n",
    "\n",
    "predicted_y = lrc.predict(test_vec)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Improved decision with lemmas, bi-grams + negations, count pos/neg words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive/negative words that are not in tone-dict, but used in -alco domain\n",
    "\n",
    "negative_nouns = {'шмурдяк', 'пойло', 'пійло', 'бурда', 'бодяга', 'бадяга', 'моча', 'лайно'}\n",
    "negative_adjs = {'незбалансований'}\n",
    "positive_nouns = {'агонь', 'вогонь', 'топ', 'топчік'}\n",
    "positive_adjs = {'насичений', 'ароматний', 'запашний', 'духм\\'яний', 'тягучий'}\n",
    "\n",
    "with open('tone-dict-uk.tsv', \"r\") as f:\n",
    "    f = csv.reader(f, delimiter=\"\\t\", quotechar='\"')\n",
    "    for word in f:\n",
    "        \n",
    "        token = word[0]\n",
    "        label = word[1]\n",
    "        pos_tag = morph.parse(token)[0].tag\n",
    "        \n",
    "        if (label == '-2' or label == '-1') and 'ADJF' in pos_tag:\n",
    "            negative_adjs.add(token)\n",
    "        elif (label == '-2' or label == '-1') and 'NOUN' in pos_tag:\n",
    "            negative_nouns.add(token)\n",
    "        elif (label == '2' or label == '1') and 'ADJF' in pos_tag:\n",
    "            positive_adjs.add(token)\n",
    "        elif (label == '2' or label == '1') and 'NOUN' in pos_tag:\n",
    "            positive_nouns.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data (data):\n",
    "    \"Returns data with processed negations and counts the freq of negative and positive tokens is text\"\n",
    "    data_w_negation = []\n",
    "    sentiment = []\n",
    "    \n",
    "    # process negation in text\n",
    "    for sent in data:\n",
    "        sent = re.sub(r'(\\bне) (\\w+)', r'\\1_\\2', sent)\n",
    "        data_w_negation.append(sent)\n",
    "\n",
    "    # count negative/positive tokens in every comment\n",
    "        tokenized_text = sent.split()\n",
    "        count_pos = 0\n",
    "        count_neg = 0\n",
    "\n",
    "        for token in tokenized_text:\n",
    "            if token in positive_adjs or token in positive_nouns:\n",
    "                count_pos += 1\n",
    "            if token in negative_adjs or token in negative_nouns:\n",
    "                count_neg += 1\n",
    "\n",
    "        sentiment.append([count_pos, count_neg])\n",
    "    sentiment = np.array(sentiment)\n",
    "        \n",
    "    return data_w_negation, sentiment\n",
    "    \n",
    "\n",
    "X_train, train_sentiment = prepare_data(X_train_lemmas)\n",
    "X_test, test_sentiment = prepare_data(X_test_lemmas)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'char', ngram_range = (1,5))\n",
    "train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "test_vec = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# add sentiment count to vector\n",
    "train_vec = np.hstack((train_vec, train_sentiment))\n",
    "test_vec = np.hstack((test_vec, test_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.77      0.79       213\n",
      "    positive       0.78      0.82      0.80       213\n",
      "\n",
      "    accuracy                           0.79       426\n",
      "   macro avg       0.79      0.79      0.79       426\n",
      "weighted avg       0.79      0.79      0.79       426\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lrc = LogisticRegression(solver='sag', max_iter = 1000, random_state=42)\n",
    "lrc.fit(train_vec, y_train)\n",
    "\n",
    "predicted_y = lrc.predict(test_vec)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7801286899942095"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(solver = 'sag', random_state=42)\n",
    "\n",
    "scores = cross_val_score(log_reg, train_vec, y_train, cv=5, scoring='f1_macro', n_jobs = -1)\n",
    "sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = {'сподобатися', 'подобатися', 'купувати', 'купити'} \n",
    "properties = {'смак', 'колір', 'запах', 'аромат', 'консистенція', 'якість', 'пляшка', 'бутилка'\n",
    "              'нотка', 'нота', 'післясмак', 'ціна', 'вигляд', 'пляшка'}\n",
    "drink_name = {'напій', 'вино', 'винчік', 'винішко', 'винцо', 'лікер', 'віскі', 'коньяк', 'коньчок',\n",
    "              'водка', 'горілка', 'ром', 'джин', 'текіла', 'самбука', 'грапа', 'граппа', 'кальвадос',\n",
    "              'пиво', 'пивас', 'пивасік', 'тонік', 'товар', 'шампанське', \n",
    "              'шампунь', 'шампусік'}\n",
    "neg_verbs = {'зкурвитися', 'погіршитися'}\n",
    "\n",
    "def has_verb_negation (text):\n",
    "    for i in range(len(text)):\n",
    "        if morph.parse(text[i])[0].normal_form in verbs and text[i-1] == 'не':\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def count_pos_words (text):\n",
    "    count = 0\n",
    "    for i in range(len(text)):\n",
    "        normal_form = morph.parse(text[i])[0].normal_form\n",
    "        if (normal_form in positive_adjs or normal_form in positive_nouns) and text[i-1] != 'не':\n",
    "            count += 1        \n",
    "    return count\n",
    "\n",
    "def count_neg_words (text):\n",
    "    count = 0\n",
    "    for i in range(len(text)):\n",
    "        normal_form = morph.parse(text[i])[0].normal_form\n",
    "        if (normal_form in negative_adjs or normal_form in negative_nouns) and text[i-1] != 'не':\n",
    "            count += 1        \n",
    "    return count\n",
    "\n",
    "def has_neg_property (text):\n",
    "    for i in range(len(text)):\n",
    "        if morph.parse(text[i])[0].normal_form in properties and morph.parse(text[i-1])[0].normal_form in negative_adjs:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_good_drink (text):\n",
    "    for i in range(len(text)):\n",
    "        if morph.parse(text[i])[0].normal_form in drink_name \\\n",
    "        and morph.parse(text[i-1])[0].normal_form in positive_adjs:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['count_neg_words'] = normalized_df['tokens'].apply(count_neg_words)\n",
    "features['count_pos_words'] = normalized_df['tokens'].apply(count_pos_words)\n",
    "features['has_verb_negation'] = normalized_df['tokens'].apply(has_verb_negation)\n",
    "features['has_neg_property'] = normalized_df['tokens'].apply(has_neg_property)\n",
    "features['is_good_drink'] = normalized_df['tokens'].apply(is_good_drink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, normalized_df['label'],\n",
    "                                                    test_size = 0.3, random_state = 42,\n",
    "                                                    stratify = normalized_df['label'])\n",
    "\n",
    "lrc1 = LogisticRegression(random_state=42, solver='sag')\n",
    "lrc1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.68      0.66       213\n",
      "    positive       0.66      0.61      0.63       213\n",
      "\n",
      "    accuracy                           0.65       426\n",
      "   macro avg       0.65      0.65      0.65       426\n",
      "weighted avg       0.65      0.65      0.65       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y = lrc1.predict(X_test)\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
