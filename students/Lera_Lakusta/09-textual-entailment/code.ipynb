{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_data (filename):\n",
    "    path = 'data/snli_1.0/'\n",
    "    with open(path + filename, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines() if '\"gold_label\": \"-\"' not in line]\n",
    "            \n",
    "    labels = [x['gold_label'] for x in data]\n",
    "    for x in data:\n",
    "        del x['gold_label']\n",
    "        del x['annotator_labels']\n",
    "        del x['captionID']\n",
    "        del x['pairID']\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = get_data('snli_1.0_train.jsonl')\n",
    "dev_data, dev_labels = get_data('snli_1.0_dev.jsonl')\n",
    "test_data, test_labels = get_data('snli_1.0_test.jsonl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Додаю усі речення з датасету у список, так їх буде швидше опрацьовувати через nlp.pipe у spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sents (data):\n",
    "    sents = []\n",
    "    for item in data:\n",
    "        sents.append(item['sentence1'])\n",
    "        sents.append(item['sentence2'])\n",
    "    return sents\n",
    "\n",
    "train_sents = prepare_sents(train_data)\n",
    "dev_sents = prepare_sents(dev_data)\n",
    "test_sents = prepare_sents(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences len: 1098734\n",
      "Dev sentences len: 19684\n",
      "Test sentences len: 19648\n"
     ]
    }
   ],
   "source": [
    "print('Train sentences len:', len(train_sents))\n",
    "print('Dev sentences len:', len(dev_sents))\n",
    "print('Test sentences len:', len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models.wrappers import FastText\n",
    "import gensim.models.wrappers.fasttext\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/wiki_news.vec', binary=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_sent_vec (sentences):\n",
    "    result = []\n",
    "    default_vec = np.ones(300)\n",
    "\n",
    "    for sent in nlp.pipe(sentences, disable=[\"ner\", \"textcat\"]):\n",
    "        word_vectors = []\n",
    "        for token in sent:\n",
    "            try:\n",
    "                word_vector = model[token.lemma_]\n",
    "                word_vectors.append(np.array(word_vector))\n",
    "            except:\n",
    "                word_vectors.append(np.array(default_vec))\n",
    "            \n",
    "        sentence_mean = np.mean(np.array(word_vectors), axis=0).reshape(-1,1)\n",
    "        result.append(sentence_mean)\n",
    "\n",
    "    return result\n",
    "\n",
    "train_vectors = get_sent_vec(train_sents)\n",
    "dev_vectors = get_sent_vec(dev_sents)\n",
    "test_vectors = get_sent_vec(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similiarity (list_of_vecs):\n",
    "    similarities = []\n",
    "    for i in range(len(list_of_vecs)):\n",
    "        if i % 2 == 0:\n",
    "            similarities.append(cosine(list_of_vecs[i], list_of_vecs[i + 1]))\n",
    "    return similarities\n",
    "\n",
    "train_features_base = np.array(get_similiarity(train_vectors)).reshape(-1,1)\n",
    "dev_features_base = np.array(get_similiarity(dev_vectors)).reshape(-1,1)\n",
    "test_features_base = np.array(get_similiarity(test_vectors)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.01      0.36      0.02       110\n",
      "   entailment       0.69      0.37      0.48      6248\n",
      "      neutral       0.39      0.36      0.38      3484\n",
      "\n",
      "     accuracy                           0.37      9842\n",
      "    macro avg       0.36      0.36      0.29      9842\n",
      " weighted avg       0.58      0.37      0.44      9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "clf = LogisticRegression(solver = 'sag',random_state=0).fit(train_features_base, train_labels)\n",
    "y_pred = clf.predict(dev_features_base)\n",
    "print(classification_report(y_pred, dev_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лексична схожість "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 29s, sys: 5min 18s, total: 28min 48s\n",
      "Wall time: 29min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "def get_linguistic_data (sentences):\n",
    "    result = []\n",
    "    for sent in nlp.pipe(sentences, disable=[\"textcat\"]):\n",
    "        sent_data = defaultdict(list)\n",
    "        \n",
    "        for i, token in enumerate(sent):\n",
    "            sent_data['words'].append(token.text.lower())\n",
    "            sent_data['lemmas'].append(token.lemma_)\n",
    "            sent_data['stems'].append(stemmer.stem(token.text))\n",
    "            sent_data['pos'].append(token.pos_)\n",
    "            sent_data['tags'].append(token.tag_)\n",
    "            sent_data['lemma->pos'].append((token.lemma_,token.pos_))\n",
    "            if token.pos_ == 'VERB' and sent[i-1].lemma_ in  ['never', 'not']:\n",
    "                sent_data['has_neg_verb'].append(1)\n",
    "        for ent in sent.ents:\n",
    "            sent_data['entities'].append(ent.text) \n",
    "            sent_data['ent_labels'].append(ent.label_)\n",
    "            \n",
    "        result.append(sent_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "lingdata_train = get_linguistic_data(train_sents)\n",
    "lingdata_dev = get_linguistic_data(dev_sents)\n",
    "lingdata_test = get_linguistic_data(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features (data, vec_similiarities):\n",
    "    features = []\n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i % 2 == 0:\n",
    "            pairs.append((data[i], data[i + 1]))\n",
    "            \n",
    "    for i, p in enumerate(pairs):\n",
    "        pair_feats = dict()\n",
    "        pair_feats['jaccard_words'] = jaccard_similarity(p[0]['words'], p[1]['words'])\n",
    "        pair_feats['jaccard_lemmas'] = jaccard_similarity(p[0]['lemmas'], p[1]['lemmas'])\n",
    "        pair_feats['jaccard_stems'] = jaccard_similarity(p[0]['stems'], p[1]['stems'])\n",
    "        pair_feats['jaccard_pos'] = jaccard_similarity(p[0]['pos'], p[1]['pos'])\n",
    "        pair_feats['jaccard_tags'] = jaccard_similarity(p[0]['tags'], p[1]['tags'])\n",
    "        if p[0]['jaccard_entities'] and p[1]['jaccard_entities']:\n",
    "            pair_feats['jaccard_entities'] = jaccard_similarity(p[0]['entities'], p[1]['entities'])\n",
    "            pair_feats['jaccard_ent_tags'] = jaccard_similarity(p[0]['ent_labels'], p[1]['ent_labels'])\n",
    "        if p[0]['has_neg_verb']:\n",
    "            pair_feats['has_neg_verb1'] = True\n",
    "        if p[1]['has_neg_verb']:\n",
    "            pair_feats['has_neg_verb2'] = True\n",
    "        # додаємо cosine similarity з минулої ітераці\n",
    "        pair_feats['cosine_sim'] = vec_similiarities[i][0]\n",
    "        features.append(pair_feats)\n",
    "        \n",
    "    return features\n",
    "\n",
    "train_features_lexic = extract_features(lingdata_train, train_features_base)\n",
    "dev_features_lexic = extract_features(lingdata_dev, dev_features_base)\n",
    "test_features_lexic = extract_features(lingdata_test, test_features_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лексична схожість + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.62      0.46      0.53      4438\n",
      "   entailment       0.52      0.52      0.52      3376\n",
      "      neutral       0.23      0.37      0.29      2028\n",
      "\n",
      "     accuracy                           0.46      9842\n",
      "    macro avg       0.46      0.45      0.44      9842\n",
      " weighted avg       0.51      0.46      0.47      9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(train_features_lexic)\n",
    "clf = LogisticRegression(solver = 'sag', random_state = 0).fit(X, train_labels)\n",
    "y_pred = clf.predict(vec.transform(dev_features_lexic))\n",
    "print(classification_report(y_pred, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лексична схожість + CatBoost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Видає кращу якість, тому буду далі працювати з ним."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier \n",
    "\n",
    "def train_catboost(X_train, X_test, y_train, y_test):\n",
    "    params = {\n",
    "        \"iterations\": 2500,\n",
    "        \"learning_rate\": 0.5,\n",
    "        \"random_seed\": 1,\n",
    "        \"od_wait\": 30,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"thread_count\": 8,\n",
    "        \"max_depth\": 10,\n",
    "    }\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test),\n",
    "        verbose=200,\n",
    "        plot=False,\n",
    "    )\n",
    "    y_pred = model.predict(X_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0612842\ttest: 1.0585410\tbest: 1.0585410 (0)\ttotal: 588ms\tremaining: 24m 29s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 1.012979669\n",
      "bestIteration = 94\n",
      "\n",
      "Shrink model to first 95 iterations.\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.62      0.48      0.54      4246\n",
      "   entailment       0.56      0.53      0.55      3562\n",
      "      neutral       0.27      0.43      0.33      2034\n",
      "\n",
      "     accuracy                           0.48      9842\n",
      "    macro avg       0.48      0.48      0.47      9842\n",
      " weighted avg       0.53      0.48      0.50      9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_lexic_df = pd.DataFrame(train_features_lexic)\n",
    "train_lexic_df.fillna(train_lexic_df.mean(), inplace= True)\n",
    "dev_lexic_df = pd.DataFrame(dev_features_lexic)\n",
    "dev_lexic_df.fillna(dev_lexic_df.mean(), inplace= True)\n",
    "ctb = train_catboost(train_lexic_df, dev_lexic_df, train_labels, dev_labels)\n",
    "\n",
    "y_pred = ctb.predict(dev_lexic_df)\n",
    "print(classification_report(y_pred, [str(x) for x in dev_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семантична схожість"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words (sent):\n",
    "    nouns, nums, verbs, advs, adjs = set(), set(), set(), set(), set()\n",
    "    \n",
    "    for word, tag in sent:\n",
    "        w = word.lower()\n",
    "        if tag == 'NOUN':\n",
    "            nouns.add(w)\n",
    "        if tag == 'NUM':\n",
    "            nums.add(w)\n",
    "        if tag == 'VERB':\n",
    "            verbs.add(w)\n",
    "        if tag == 'ADV':\n",
    "            advs.add(w)\n",
    "        if tag == 'ADJ':\n",
    "            adjs.add(w)\n",
    "            \n",
    "    return nouns, nums, verbs, advs, adjs\n",
    "\n",
    "def get_unique_words (l1, l2):\n",
    "    nouns1, nums1, verbs1, advs1, adjs1 = get_words(l1)\n",
    "    nouns2, nums2, verbs2, advs2, adjs2 = get_words(l2)\n",
    "\n",
    "    return (nouns1 - nouns2, nouns2-nouns1), (nums1 - nums2,nums2-nums1), \\\n",
    "        (verbs1 - verbs2, verbs2 - verbs1),  (advs1 - advs2, advs2 - advs1),  (adjs1 - adjs2, adjs2 - adjs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  \n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "concepts = dict()\n",
    "\n",
    "def calc_semantic_rels_wn (l1, l2):\n",
    "    \n",
    "    if len(l1) and len(l2):\n",
    "        count_ant, count_syn, count_hypernym, count_hyponym  = 0, 0, 0, 0\n",
    "        pairs = itertools.product(l1, l2)\n",
    "            \n",
    "        for p in pairs:\n",
    "            word1, word2 = p[0], p[1]\n",
    "            if word1 not in concepts:\n",
    "                syn, ant, hypernym, hyponym = set(), set(), set(), set()\n",
    "                synsets = wordnet.synsets(word1)\n",
    "                \n",
    "                for synset in synsets:\n",
    "                    for lemma in synset.lemmas():\n",
    "                        syn.add(lemma.name())   \n",
    "                        for lemma in lemma.antonyms():\n",
    "                            ant.add(lemma.name())\n",
    "                    for x in synset.hypernyms():\n",
    "                        for lemma in x.lemmas():\n",
    "                            hypernym.add(lemma.name())\n",
    "                    for x in synset.hyponyms():\n",
    "                        for lemma in x.lemmas():\n",
    "                            hyponym.add(lemma.name())\n",
    "                            \n",
    "                concepts[word1] = {\n",
    "                    'synonym': syn,\n",
    "                    'antonym': ant,\n",
    "                    'hypernym': hypernym,\n",
    "                    'hyponym': hyponym,\n",
    "                }\n",
    "            syn = concepts[word1]['synonym']\n",
    "            ant = concepts[word1]['antonym']\n",
    "            hypernym = concepts[word1]['hypernym']\n",
    "            hyponym = concepts[word1]['hyponym']\n",
    "\n",
    "            if word2 in syn:\n",
    "                count_syn += 1\n",
    "            if word2 in ant:\n",
    "                count_ant += 1\n",
    "            if word2 in hypernym:\n",
    "                count_hypernym += 1\n",
    "            if word2 in hyponym:\n",
    "                count_hyponym += 1\n",
    "        return count_syn, count_ant, count_hypernym, count_hyponym\n",
    "    \n",
    "    return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 1.89 s, total: 22.9 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def extract_features_wn (data):\n",
    "    features_list = []\n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i % 2 == 0:\n",
    "            pairs.append((data[i], data[i + 1]))   \n",
    "        \n",
    "    for i, p in enumerate(pairs):\n",
    "            \n",
    "        (unique_nn1, unique_nn2), (unique_nums1, unique_nums2), \\\n",
    "        (unique_verbs1, unique_verbs2), (unique_advs1, unique_advs2), \\\n",
    "        (unique_adjs1, unique_adjs2) = get_unique_words(p[0]['lemma->pos'], p[1]['lemma->pos'])\n",
    "        \n",
    "        nn_synon, nn_anton, nn_hyper, nn_hypo = calc_semantic_rels_wn(unique_nn1, unique_nn2) \n",
    "        num_synon, num_anton, num_hyper, num_hypo  = calc_semantic_rels_wn(unique_nums1, unique_nums2)\n",
    "        verb_synon, verb_anton, verb_hyper, verb_hypo  = calc_semantic_rels_wn(unique_verbs1, unique_verbs2)\n",
    "        adv_synon, adv_anton, adv_hyper, adv_hypo = calc_semantic_rels_wn(unique_advs1, unique_advs2) \n",
    "        adj_synon, adj_anton, adj_hyper, adj_hypo  = calc_semantic_rels_wn(unique_adjs1, unique_adjs2) \n",
    "            \n",
    "        features_list.append({\n",
    "            \"nn_synon\": nn_synon,\n",
    "            \"nn_anton\": nn_anton,\n",
    "            \"nn_hyper\": nn_hyper,\n",
    "            \"nn_hypo\": nn_hypo,\n",
    "            \"num_synon\": num_synon,\n",
    "            \"num_anton\": num_anton,\n",
    "            \"num_hyper\": num_hyper,\n",
    "            \"num_hypo \": num_hypo ,\n",
    "            \"verb_synon\": verb_synon,\n",
    "            \"verb_anton\": verb_anton,\n",
    "            \"verb_hyper\": verb_hyper,\n",
    "            \"verb_hypo\": verb_hypo,\n",
    "            \"adv_synon\": adv_synon,\n",
    "            \"adv_anton\": adv_anton,\n",
    "            \"adv_hyper\": adv_hyper,\n",
    "            \"adv_hypo\": adv_hypo,\n",
    "            \"adj_synon\": adj_synon,\n",
    "            \"adj_anton\": adj_anton,\n",
    "            \"adj_hyper\": adj_hyper,\n",
    "            \"adj_hypo\": adj_hypo,\n",
    "        })\n",
    "    return features_list\n",
    "\n",
    "train_features_wn = extract_features_wn(lingdata_train)\n",
    "dev_features_wn = extract_features_wn(lingdata_dev)\n",
    "test_features_wn = extract_features_wn(lingdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def merge_features (features1, features2):\n",
    "    features = copy.deepcopy(features1)\n",
    "    for i, f in enumerate(features2):\n",
    "        for k,v in f.items():\n",
    "            features[i][k] = v\n",
    "    return features\n",
    "\n",
    "train_features_final = merge_features(train_features_lexic, train_features_wn)\n",
    "dev_features_final = merge_features(dev_features_lexic, dev_features_wn)\n",
    "test_features_final = merge_features(test_features_lexic, test_features_wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9959192\ttest: 0.9914860\tbest: 0.9914860 (0)\ttotal: 695ms\tremaining: 28m 56s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.9109542333\n",
      "bestIteration = 56\n",
      "\n",
      "Shrink model to first 57 iterations.\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.57      0.57      0.57      3244\n",
      "   entailment       0.60      0.65      0.62      3150\n",
      "      neutral       0.49      0.46      0.47      3430\n",
      "\n",
      "     accuracy                           0.56      9824\n",
      "    macro avg       0.56      0.56      0.56      9824\n",
      " weighted avg       0.55      0.56      0.55      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_features_final)\n",
    "train_df.fillna(float('inf'), inplace= True)\n",
    "dev_df = pd.DataFrame(dev_features_final)\n",
    "dev_df.fillna(float('inf'), inplace= True)\n",
    "test_df = pd.DataFrame(test_features_final)\n",
    "test_df.fillna(float('inf'), inplace= True)\n",
    "\n",
    "ctb = train_catboost(train_df, dev_df, train_labels, dev_labels)\n",
    "\n",
    "y_pred = ctb.predict(test_df)\n",
    "print(classification_report(y_pred, [str(x) for x in test_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1170\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                nn_anton\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1167\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                jaccard_stems\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.60%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0881\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                cosine_sim\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.46%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0815\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                jaccard_pos\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.36%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0749\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                verb_hyper\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.39%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0747\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                nn_hyper\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0651\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                jaccard_lemmas\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.35%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0608\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                nn_synon\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.92%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0503\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                jaccard_words\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0491\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                jaccard_tags\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0374\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                verb_anton\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.26%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0359\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                verb_synon\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.70%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0277\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                adj_anton\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.20%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0250\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                nn_hypo\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0246\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                adj_synon\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0206\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                verb_hypo\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0178\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                adj_hypo\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0124\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                adj_hyper\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.48%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0098\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                num_synon\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.59%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0057\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                adv_anton\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.59%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 8 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator='<catboost.core.CatBoostClassifier object at 0x1cd081b0f0>', description='CatBoost feature importances; \\nvalues are numbers 0 <= x <= 1; all values sum to 1.', error=None, method='feature importances', is_regression=False, targets=None, feature_importances=FeatureImportances(importances=[FeatureWeight(feature='nn_anton', weight=0.11698576, std=None, value=None), FeatureWeight(feature='jaccard_stems', weight=0.116715424, std=None, value=None), FeatureWeight(feature='cosine_sim', weight=0.08810216, std=None, value=None), FeatureWeight(feature='jaccard_pos', weight=0.08154936, std=None, value=None), FeatureWeight(feature='verb_hyper', weight=0.07488765, std=None, value=None), FeatureWeight(feature='nn_hyper', weight=0.07467543, std=None, value=None), FeatureWeight(feature='jaccard_lemmas', weight=0.06513084, std=None, value=None), FeatureWeight(feature='nn_synon', weight=0.060776576, std=None, value=None), FeatureWeight(feature='jaccard_words', weight=0.050341196, std=None, value=None), FeatureWeight(feature='jaccard_tags', weight=0.04911399, std=None, value=None), FeatureWeight(feature='verb_anton', weight=0.037362333, std=None, value=None), FeatureWeight(feature='verb_synon', weight=0.035872966, std=None, value=None), FeatureWeight(feature='adj_anton', weight=0.02773928, std=None, value=None), FeatureWeight(feature='nn_hypo', weight=0.025026737, std=None, value=None), FeatureWeight(feature='adj_synon', weight=0.02458487, std=None, value=None), FeatureWeight(feature='verb_hypo', weight=0.020621818, std=None, value=None), FeatureWeight(feature='adj_hypo', weight=0.017761892, std=None, value=None), FeatureWeight(feature='adj_hyper', weight=0.012369866, std=None, value=None), FeatureWeight(feature='num_synon', weight=0.009792226, std=None, value=None), FeatureWeight(feature='adv_anton', weight=0.0057013463, std=None, value=None)], remaining=8), decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eli5.catboost import explain_weights_catboost\n",
    "\n",
    "explain_weights_catboost(ctb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Висновки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектори не зовсім якісно справляють із задачею знайдення логічних зв'язків (мій бейзлайн). У мене не було великих сподівань на них, але минулої лекції Сєва сказав, що це може бути гарний бейзлайном, тому я вирішила спробувати.  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Моя перша спроба дістати семантичні фічі базувалася на ConceptNet. Усього у тексті більше 28.000 унікальних лем самостійних частин мови, для такої кількості надіслати запити до conceptnet за тиждень нереально. Я пробувала різні оптимізації коду, у тому числі мемоізацію, але це не пришвидшило збирання концептів, тому довелося використовувати  WordNet. \n",
    "\n",
    "На жаль, на покращення коду з концептнет у мене пішло велика кількість часу, тому не встигла зробити ті граматичні фічі, які збиралася."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щодо роботи WordNet, то на відміну від ConceptNet, де можно було відфільтрувати сумнівні зв'язки у частинах концепту, використовуючи оцінку (або ступінь довіри автору, не пам'ятаю точно як точно називається), то у першому дуже шумні дані. Ось наприклад гіпероніми для слова cat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](screenshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
