В якості датасету я взяв коментарі до продуктів з категорії "Велика побутова техніка".
Базове рішення показало дуууже пагані результати.
В якості покращень я поступово додавав:
1)лемматизацію
2)тональний словник
3)застосування біграм

Лемматизація суттєво не вплинула на результати і здається в чомусь їх погіршила навіть.
Думаю це сталося через те, що перший кандидат із ряду який пропонує pymorphy може бути іншим словом.

Використання тонального словника допомогло більш вдало розмітити датасет,
і метрики помітно виросли(хоча результат все ще не дуже).

Застосування біграм теж схоже вплинуло позитивно й деякі метрики покращилися.
Думаю в данному випадку вони виявилися більш показовими й однозначними маркерами ніж уніграми.

Основна проблема в цьому завданні - це вхідна інформація.
У відгуках дуже багато шумів, користувачі не вказують цифрову оцінку, або вказують її врозріз із текстом відгука.
Також люди вносять інформацію яка при грубій обробці вносить плутанину. 
Наприклад пишуть у поле для мінусів/плюсів товару: не виявлено/нема/etc.
Враховуючи що найбільший приріст в моєму варіанті рішення дало використання словника для класифікації датасету,
найбільш ефективним шляхом для покращення буде розширення словника маркерних слів та словосполучень.
Також необхідно більш ретельно очищати й нормалізувати вхідні тексти виявляючи й відкидаючи різного роду питання 
та безсенсові відгуки(напркилад просто "добрий день"). 

Також я думаю, що вибір для датасету конкретної підкатегорії(наприклад виключно пральні машинки чи холодильники)
можоиво покращив би результат, оскільки в текстах зустрічалися б менш різноманітні слова(описання дефектів товару, тощо).
Проте в такому випадку міг вийти недостатньо великий датасет.

Basic version:

Counter({'pos': 3413, 'neut': 1988, 'neg': 382})
              precision    recall  f1-score   support

         neg       0.05      0.09      0.06       113
        neut       0.36      0.43      0.39       468
         pos       0.65      0.52      0.58       865

    accuracy                           0.46      1446
   macro avg       0.35      0.34      0.34      1446
weighted avg       0.51      0.46      0.48      1446

Lemmatized version:

Counter({'pos': 3413, 'neut': 1988, 'neg': 382})
Cross-validation lemmatization to basic:  [0.32328426 0.31225024 0.29492741 0.29102364 0.30289889]
              precision    recall  f1-score   support

         neg       0.04      0.20      0.07        74
        neut       0.41      0.42      0.41       517
         pos       0.66      0.43      0.52       855

    accuracy                           0.41      1446
   macro avg       0.37      0.35      0.33      1446
weighted avg       0.54      0.41      0.46      1446

With tone dict:

Counter({'pos': 4084, 'neg': 1038, 'neut': 661})
Cross-validation tone dict to lemmatization:  [0.31647422 0.3118711  0.32216494 0.31796435 0.30512365]
              precision    recall  f1-score   support

         neg       0.20      0.47      0.28       239
        neut       0.35      0.39      0.37       157
         pos       0.77      0.51      0.61      1050

    accuracy                           0.49      1446
   macro avg       0.44      0.46      0.42      1446
weighted avg       0.63      0.49      0.53      1446

With bigrams:

Counter({'pos': 4084, 'neg': 1038, 'neut': 661})
Cross-validation bigrams to tone dict:  [0.35621313 0.34438422 0.31310037 0.32835556 0.34864846]
              precision    recall  f1-score   support

         neg       0.27      0.23      0.25       265
        neut       0.43      0.28      0.34       164
         pos       0.75      0.83      0.79      1017

    accuracy                           0.66      1446
   macro avg       0.48      0.45      0.46      1446
weighted avg       0.63      0.66      0.64      1446
