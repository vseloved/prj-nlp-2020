# Генерація перефразувань

## Завдання

Ваше завдання цього тижня - розробити seq2seq модель для генерації перефразувань на рівні символів, слів чи bpe. Ось корисні посилання:
- <https://keras.io/examples/lstm_seq2seq/>
- <https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html>
- <https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/>

Для тренування використайте [скорочений варіант корпусу Opusparcus](opusparcus_v2.zip). Врахуйте, що кожну пару перефразувань можна використовувати для тренування в обидва боки.

**Для довідки.** Оригінальний корпус [Opusparcus](https://korp.csc.fi/download/opusparcus/). Це корпус перефразувань, побудований на основі різних субтитрів до тих самих фільмів. Тренувальна вибірка у корпусі дуже велика (40 млн прикладів для англійської мови), але автоматично проанотована. Більшість перефразувань надто примітивні, тож скорочений варіант корпусу містить перші сто тисяч прикладів з високою відстанню редагування (значення останньої колонки; див. README).

## Метрики

Для оцінки якості генерації перефразувань використаймо дві метрики:
1. ROUGE-L, виміряна на dev-частині корпусу. Можна скористатися готовою імплементацією метрики з бібліотеки [py-rouge](https://pypi.org/project/py-rouge/).
2. Ручна оцінка якості. Виберіть 50 випадкових речень із dev-частини корпусу, згенеруйте перефразування для них та підготуйте Google-таблицю. Інший студент у групі оцінить якість генерації.

## Оцінювання

За побудову моделі - 80 балів. За анотування 50-ти прикладів генерації іншого студента - 20 балів.

Крайній термін: 30.05.2020
